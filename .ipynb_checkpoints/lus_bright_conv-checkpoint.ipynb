{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YapYOP_srblT"
   },
   "source": [
    "# LUS-CS433\n",
    "\n",
    "TODO general project description + do a readme\n",
    "\n",
    "For this task, we decide to explore a supervised learning approach, using CNNs.\n",
    "Our initial idea is the following: to train a binary classifier ideally able to emulate a clinician's judgement, namely telling us wether an image taken from an LUS video of a patient site should be selected or not.\n",
    "\n",
    "TODO explain why classifier is multi site, wether we should do multiple classifiers (one per site)\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TZoIvD9nJuD1"
   },
   "outputs": [],
   "source": [
    "from encoders import CnnEncoder\n",
    "from cumulator import base\n",
    "from plots import plot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBJ5-wpVMirO"
   },
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "TODO some data exploration (number of images, videos, formats, etc)\n",
    "\n",
    "explain the concept of patient site\n",
    "#### Image shape\n",
    "We first check whether images in the dataset have all the same shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "butterfly_img_path = glob.glob(\"data/Ultrason_butterflynetwork/*.png\")\n",
    "vid_img_path = glob.glob(\"data/Ultrason_butterflynetwork/*.mp4\")\n",
    "\n",
    "def count_formats(path_list):\n",
    "    counts = {}\n",
    "    for path in path_list:\n",
    "        key = cv2.imread(path).shape\n",
    "        if key not in counts.keys():\n",
    "            counts[key] = 0\n",
    "        else:\n",
    "            counts[key] = counts[key] + 1\n",
    "    return counts\n",
    "\n",
    "def count_formats_vid(vid_list):\n",
    "    counts = {}\n",
    "    video = cv2.VideoCapture(vid_path)\n",
    "    while(video.isOpened()):\n",
    "        success, img = video.read()\n",
    "\n",
    "        if success == False :\n",
    "            video.release()\n",
    "            continue\n",
    "        key = img.shape\n",
    "        if key not in counts.keys():\n",
    "            counts[key] = 0\n",
    "        else:\n",
    "            counts[key] = counts[key] + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1080, 791, 3): 3453, (1080, 790, 3): 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butterfly_format = count_formats(butterfly_img_path)\n",
    "butterfly_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vid_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1c4f2da30949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvid_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_formats_vid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvid_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b394f00c7a1a>\u001b[0m in \u001b[0;36mcount_formats_vid\u001b[0;34m(vid_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_formats_vid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vid_path' is not defined"
     ]
    }
   ],
   "source": [
    "vid_format = count_formats_vid(vid_img_path)\n",
    "vid_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is not everywhere the case, so we can't infer that we'll always the same type of images. It seems that our videos have all the same shapes. However most of our images in the butterfly dataset come from videos.\n",
    "\n",
    "From this analysis, we also get that we have 3447 images in the butterfly dataset, and 1265 videos. While in the LUS_neg dataset, we have 1243 images.\n",
    "\n",
    "Let's inspect more precisely our videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Ultrason butterflynetwork\\1_100_QLD.mp4\n"
     ]
    }
   ],
   "source": [
    "one_vid_path = vid_img_path[0]\n",
    "print(one_vid_path)\n",
    "def count_formats_vid(vid_path):\n",
    "    counts = {}\n",
    "    video = cv2.VideoCapture(vid_path)\n",
    "    while(video.isOpened()):\n",
    "        success, img = video.read()\n",
    "        \n",
    "        if success == False :\n",
    "            print('problem')\n",
    "            continue\n",
    "        key = img.shape\n",
    "        if key not in counts.keys():\n",
    "            counts[key] = 0\n",
    "        else:\n",
    "            counts[key] = counts[key] + 1\n",
    "    print(\"hello\")\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_formats_vid(one_vid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I couldn't run the cell, but we should have seen that even in one video, the format could sometimes change by one or 2 pixels. This explains why we have different images shape in our images dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is our dataset distributed according to sites?\n",
    "When a clinician performs a lung ultrasound (LUS) examination, he generally acquires several images on different thoracic sites. Each lung is partitioned in four quadrants (anterior or posterior and superior or inferior). They also perform one lung ultrasound on the side of the lung which implies that a patient could have up to 10 different videos of his lungs.\n",
    "\n",
    "Here we look how is our data distributed according to each site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sites(path):\n",
    "    site_nb = {}\n",
    "    for data_path in path:\n",
    "        site_ext = data_path.split('_')[2]#get the site, but could contain extension\n",
    "        site = site_ext.split(\".\")[0]#removes the extension\n",
    "        \n",
    "        if site not in site_nb.keys():\n",
    "            site_nb[site] = 1\n",
    "        else:\n",
    "            site_nb[site] = site_nb[site] + 1\n",
    "    return site_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Ultrason_butterflynetwork/1.46_QAIG_1.png\n",
      "data/Ultrason_butterflynetwork/1.156_QASD_1.png\n"
     ]
    }
   ],
   "source": [
    "#Some files had misspellings, we take fix this here by modifying their names:\n",
    "old_path = 'data/Ultrason_butterflynetwork/1.46_QAiG_1.png'\n",
    "new_path = old_path.replace(\"QAiG\", \"QAIG\", 1)\n",
    "print(new_path)\n",
    "os.rename(old_path, new_path)\n",
    "\n",
    "old_path2 = 'data/Ultrason_butterflynetwork/1.156_QASD-1.png'\n",
    "new_path2 = old_path2.replace(\"QASD-1\", \"QASD_1\")\n",
    "print(new_path2)\n",
    "os.rename(old_path2, new_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QAID': 369,\n",
       " 'QAIG': 350,\n",
       " 'QASD': 399,\n",
       " 'QASG': 385,\n",
       " 'QASP': 1,\n",
       " 'QLD': 229,\n",
       " 'QLG': 225,\n",
       " 'QPID': 354,\n",
       " 'QPIG': 349,\n",
       " 'QPSD': 397,\n",
       " 'QPSG': 382,\n",
       " 'UNK': 17}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butterfly_img_path = glob.glob(\"data/Ultrason_butterflynetwork/*.png\")\n",
    "butterfly_site_nb = dict(sorted(get_sites(butterfly_img_path).items()))\n",
    "butterfly_site_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide that UNK (don't know what it refers to) and QASP (Antérieur Supérieur Postérieur?) images are outliers so we don't consider them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    butterfly_site_nb.pop('UNK')\n",
    "    butterfly_site_nb.pop('QASP')\n",
    "except:\n",
    "    print('It seems they were already removed')\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(butterfly_site_nb.keys(), butterfly_site_nb.values())\n",
    "plt.title('Nb images per site (Butterflynetwork)')\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.xlabel('Sites')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is more or less uniformly distributed, we have around 350-400 images per site. There are just the QLD and QLG sites that have around 220 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Ultrason_butterflynetwork/1.195_QLG.mp4\n",
      "data/Ultrason_butterflynetwork/1.184_QAID_1.mp4\n"
     ]
    }
   ],
   "source": [
    "#Same problem here as before:\n",
    "old_path = 'data/Ultrason_butterflynetwork/1.195_QlG.mp4'\n",
    "new_path = old_path.replace(\"QlG\", \"QLG\", 1)\n",
    "print(new_path)\n",
    "os.rename(old_path, new_path)\n",
    "\n",
    "old_path2 = 'data/Ultrason_butterflynetwork/1.184_QAiD_1.mp4'\n",
    "new_path2 = old_path2.replace(\"QAiD\", \"QAID\")\n",
    "print(new_path2)\n",
    "os.rename(old_path2, new_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QAID': 129,\n",
       " 'QAIG': 97,\n",
       " 'QASD': 36,\n",
       " 'QASG': 47,\n",
       " 'QLD': 294,\n",
       " 'QLG': 319,\n",
       " 'QPAG': 2,\n",
       " 'QPID': 156,\n",
       " 'QPIG': 128,\n",
       " 'QPSD': 34,\n",
       " 'QPSG': 22,\n",
       " 'UNK': 2}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_img_path = glob.glob(\"data/Ultrason_butterflynetwork/*.mp4\")\n",
    "butterfly_video_site_nb = dict(sorted(get_sites(vid_img_path).items()))\n",
    "butterfly_video_site_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Same thing here, we remove UNK videos and QPAG videos as their number is to low.\n",
    "try:\n",
    "    butterfly_video_site_nb.pop('UNK')\n",
    "    butterfly_video_site_nb.pop('QPAG')\n",
    "except:\n",
    "    print('It seems they were already removed')\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(butterfly_video_site_nb.keys(), butterfly_video_site_nb.values())\n",
    "plt.title('Nb videos per site (Butterflynetwork)')\n",
    "plt.ylabel(\"Number of videos\")\n",
    "plt.xlabel('Sites')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, we have a lot more QLD and QLG videos. We know that experts didn't extract an image from every videos or could extract multiple from one. That's why we have this different between the images distribtution and the video distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many patients and videos per patient do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patients_videos(vid_path_list):\n",
    "    \"\"\"\n",
    "    vid_path_list: list of paths of videos\n",
    "    Returns: Dictionnary with patient number (name) and number of videos associated to him \n",
    "    \"\"\"\n",
    "    patients = {}\n",
    "    for path in vid_path_list:\n",
    "        splitted_path = path.split('_') #split the path\n",
    "        patient = splitted_path[0][-1] + \"_\" + splitted_path[1] #extract the patient's number\n",
    "        if patient not in patients.keys():\n",
    "            patients[patient] = 1\n",
    "        else:\n",
    "            patients[patient] = patients[patient] + 1\n",
    "    return patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 193 patients\n"
     ]
    }
   ],
   "source": [
    "nb_patients_videos = patients_videos(vid_img_path)\n",
    "print(f\"We have {len(nb_patients_videos.keys())} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(nb_patients_videos.values(), bins = max(nb_patients_videos.values()))\n",
    "plt.title('Number of patients with x videos dedicated to them')\n",
    "plt.xlabel('Number of videos')\n",
    "plt.ylabel('Number of patients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing this, we want to know how many patients have actually videos from each site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sites_per_patient_videos(vid_path_list):\n",
    "    \"\"\"\n",
    "    vid_path_list: list of paths of videos\n",
    "    Returns: Dictionnary with patient number (name) and all sites that were performed on him\n",
    "    \"\"\"\n",
    "    patients = {}\n",
    "    for path in vid_path_list:\n",
    "        splitted_path = path.split('_') #get the patient and the\n",
    "        patient = splitted_path[0][-1] + \"_\" + splitted_path[1]\n",
    "        \n",
    "        site = splitted_path[2] #get the name of the video\n",
    "        site = site.split(\".\")[0] #removes the extension\n",
    "        \n",
    "        if patient not in patients.keys():\n",
    "            patients[patient] = set([site])\n",
    "        else:\n",
    "            patients[patient].add(site)\n",
    "    return patients\n",
    "\n",
    "sites_per_patient_dic = sites_per_patient_videos(vid_img_path)\n",
    "nb_sites_per_patient = { k : len(v) for k,v in sites_per_patient_dic.items()}\n",
    "\n",
    "plt.hist(nb_sites_per_patient.values(), bins = max(nb_sites_per_patient.values()))\n",
    "plt.title('Number of patients having x sites available on video')\n",
    "plt.xlabel('Number of sites')\n",
    "plt.ylabel('Number of patients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost no patient has a video for each site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5jpXz_R_cP8"
   },
   "source": [
    "## 2. Creating Datasets\n",
    "\n",
    "In order to train our classifier, we require an image dataset with two different kind of labels:\n",
    "\n",
    "* \"True\" images: selected by expert clinicians, these images are known to well represent a patient site and its useful features (such as pleural line, A-lines, B-lines ... etc).\n",
    "* \"False\" images: taken from an LUS video, these images were not selected by an expert clinicians thus they do not represent the perfect shot for a given patient site (image can be blurry, features could be missing or angle shot can be weird and thus image is difficult to interpret).\n",
    "\n",
    "Our initial data contains roughly a thousand expert selected images, which intuitively form our \"True\" labeled images. However, we do not have any image that would correspond to our \"False\" label. Therefore we have to generate them ourselves. We use the following methodology to accomplish our goal:\n",
    "\n",
    "For a given \"True\" image, we iterate through images from its corresponding video. For each image, we compute a similarity value to the expert image and select the image as a False-labelled one based on this criteria. We require that the expert image and the selected image are not too similar to each other (since it would make the classification too hard), yet they should not be too different either (in order to better simulate the clinician options when selecting the best image). The details of the similarity metric are explained in later sections. \n",
    "\n",
    "\n",
    "### 2.1 Creating file folders\n",
    "\n",
    "First of all, we need to define a function that creates folders that will contain our data. These follow a specific architecture that enables us to use PyTorch Datasets and Dataloaders classes conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "CAq_xxIcQAHU"
   },
   "outputs": [],
   "source": [
    "def make_directory(path):\n",
    "    \"\"\"\n",
    "    Create PyTorch compatible folder architecture\n",
    "    \"\"\"\n",
    "    ! mkdir $path\n",
    "    for kind in ['train/', 'val/']:\n",
    "        sub_path = path + kind\n",
    "        ! mkdir $sub_path\n",
    "        for label in ['true', 'false']:\n",
    "            full_path = sub_path + label\n",
    "            ! mkdir $full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Image preprocessing\n",
    "\n",
    "We start by creating a mask that we will use to crop images, getting rid of features on the images (such as the scale, or name of site) that could otherwise perturb the training.\n",
    "The mask has been crafted manually, as we did not have time to implement automatic edge detrection or similar mechanism to automate the process. We also define a couple functions for common preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main mask used to capture the relevant portion of LUS images. Crafted manually. Is [1,1,1] where image is relevant\n",
    "\n",
    "# Image dimensions\n",
    "nb_rows = 1080\n",
    "nb_cols = 792\n",
    "nb_channels = 3\n",
    "\n",
    "mask = np.zeros([nb_rows, nb_cols, nb_channels])\n",
    "\n",
    "# Filling mask\n",
    "for row in range(nb_rows):\n",
    "    for col in range(nb_cols):\n",
    "        # Delimitations of the cone like portion of a LUS image\n",
    "        if row > 25 and row < 1010 and col < 762 and (-4/5 * row + 293) < col and (4/5*row) + nb_cols-293 > col:\n",
    "            mask[row, col] = [1,1,1]\n",
    "\n",
    "mask = mask.astype('uint8')\n",
    "            \n",
    "            \n",
    "def resize_crop(image, mask, nb_cols=792, nb_rows=1080):\n",
    "    \"\"\"\n",
    "    Resize image and apply mask\n",
    "    \"\"\"\n",
    "    masked_img = cv2.resize(image, (nb_cols, nb_rows))*mask\n",
    "    return masked_img\n",
    "\n",
    "\n",
    "def read_crop(image_path, mask, nb_cols=792, nb_rows=1080):\n",
    "    \"\"\"\n",
    "    Read image, resize to given dimensions and apply mask.\n",
    "    \n",
    "    Returns: image name, image\n",
    "    \"\"\"\n",
    "    # Reading and masking image\n",
    "    cv2_img = cv2.imread(image_path)\n",
    "    masked_img = resize_crop(cv2_img, mask, nb_cols, nb_rows)\n",
    "\n",
    "    # Selecting image name from path name\n",
    "    img_name = os.path.split(image_path)[-1]\n",
    "    \n",
    "    return img_name, masked_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Obtaining Test images\n",
    "\n",
    "Before any dataset generation, we need to select and exclude a few images and video for our testing purposes. We selected ten patient ids by hand. These patients present the advantage of having images and corresponding videos for most of the sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rewrite simply\n",
    "# Create test data folder\n",
    "! mkdir data/test\n",
    "! mkdir data/test/test\n",
    "\n",
    "# Temporary text file used for copying data purposes\n",
    "! > test_data.txt\n",
    "\n",
    "# patient ids used for the test set\n",
    "test_ids = ['.151_','.136_', '.117_', '.45_', '.16_', '.77_', '.164_', '.57', '.36_', '.157_']\n",
    "\n",
    "files_path = glob.glob(\"data/Ultrason_butterflynetwork/*\")\n",
    "test_files = [file for file in files_path if any([(id_ in file) for id_ in test_ids])]\n",
    "\n",
    "file = open(\"test_data.txt\", \"w\")\n",
    "for test_file in test_files:\n",
    "    file.write(test_file+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for file in $(cat test_data.txt); \n",
    "do\n",
    "cp $file data/test/test/\n",
    "done\n",
    "rm test_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Obtaining Training and Validation images\n",
    "\n",
    "We now proceed to dataset generation. First let us define our image similarity mechanism. \n",
    "\n",
    "#### 2.2.1 Similarity between two images\n",
    "\n",
    "The similarity measure was inspired to us by ([Yang et al.](https://arxiv.org/abs/1706.04737), 2017). The idea is to encode a images to lesser dimensional format, reshape them as vector, and finally compute their cosine similarity. The encoding process is necessary to reduce complexity and also avoid potential harm caused by the curse of dimensionality (TODO Prove this). Encoding can be done in various ways. In our project, we selected two methods:\n",
    "* Computing the mean brightness for a couple subparts of the image, in a convolutional fashion.\n",
    "* Using a pretrained CNN model (resnet18) stripped from its last layers, as an encoder\n",
    "\n",
    "The function ```create_brightness_matrix``` implements the first method TODO\n",
    "Second method TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "LzgnfG9xH3He"
   },
   "outputs": [],
   "source": [
    "def create_brightness_matrix(img, mask=mask, side_length=200):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    img = resize_crop(img, mask)\n",
    "    # half_side determines the number of pixels per step of the \"convolution\"\n",
    "    half_side = int(side_length / 2)\n",
    "    nb_rows = img.shape[0] // half_side\n",
    "    nb_cols = img.shape[1] // half_side\n",
    "    matrix = np.zeros((nb_rows, nb_cols))\n",
    "    \n",
    "    for i in range(nb_rows) :\n",
    "        for j in range(nb_cols) :\n",
    "            matrix[i, j] = np.mean(img[half_side * i : half_side * i + side_length, half_side * j : half_side * j + side_length]) / 3\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors of the same size\n",
    "    \"\"\"\n",
    "    return np.dot(vector1,vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Generating datasets\n",
    "\n",
    "Armed with an encoder and a similarity measure, we can now tackle the image selection problem. We will proceed as described in Section 2.\n",
    "\n",
    "The function ```generate_datasets``` iterates through images present in the given folder, namely the Ultrason butterflynetwork one. For each of these expert selected image, the function tries to find a corresponding (same patient, same site) video through the function ```find_video```. If such video is found, the ```extract_similar_image``` function then iterates through the video, computing the similarity measure between the video's frames and the expert image, in a sequential fashion. If a given frame satisfies the similarity criterias (not too different, not too similar), then both the expert image and the selected frames are saved in their respective folder, namely the True and False labelled ones.\n",
    "\n",
    "Whenever a frame does not satisfy the similarity criterias, we do not compute similarity for the next couples frames because these are likely to give the same results considering there is very little change between two successive images in a video. This allows us to slightly reduce time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO hyperparam search on these ?\n",
    "LAST_SEEN_NB_FRAMES = 40\n",
    "\n",
    "\n",
    "def generate_dataset(read_path, write_path, encoder, min_similarity, max_similarity, nb_samples=300 , train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Creates samples for training and validation sets, using an image similarity criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    images_path = read_path + '.png'\n",
    "    images_path = np.random.permutation(glob.glob(images_path))\n",
    "    images_path = [img for img in images_path if not any([(id_ in img) for id_ in test_ids])] # ignore images used for testing\n",
    "    \n",
    "    videos_path = read_path + '.mp4'\n",
    "    videos_path = set(glob.glob(videos_path)) # transformed to set for faster queries\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    # Writing training and validation images to respective folders\n",
    "    for image_path in images_path:\n",
    "        # Finding corresponding video\n",
    "        success, video_path = find_video(image_path, videos_path)\n",
    "        if success == True:\n",
    "            if sample_count >= nb_samples:\n",
    "                break\n",
    "            else:\n",
    "                count = extract_similar_image(image_path, video_path, f\"{write_path}train/\", encoder, min_similarity, max_similarity)\n",
    "                sample_count += count\n",
    "    \n",
    "    # Moving some images to to validation folder\n",
    "    nb_samples_val = int(sample_count - (sample_count * train_ratio)) # mandatory due to floating point precision\n",
    "    move_to_val(f\"{write_path}train/\", f\"{write_path}val/\", nb_samples_val)\n",
    "    \n",
    "    if sample_count < nb_samples:\n",
    "        print(f\"Could not obtain enough samples: \\nObtained : {sample_count} \\nDemanded: {nb_samples}\")\n",
    "             \n",
    "            \n",
    "def move_to_val(img_path, val_path, nb_samples_val):\n",
    "    \"\"\"\n",
    "    Move nb_samples_val images from the img_path folder to the val_path folder\n",
    "    \"\"\"\n",
    "    img_to_move_false = glob.glob(img_path+'false/*')[:nb_samples_val]\n",
    "    img_to_move_true = glob.glob(img_path+'true/*')[:nb_samples_val]\n",
    "    val_path_false = val_path + 'false'\n",
    "    val_path_true = val_path + 'true'\n",
    "    \n",
    "    for file_false, file_true in zip(img_to_move_false, img_to_move_true):\n",
    "        !mv $file_false $val_path_false\n",
    "        !mv $file_true $val_path_true\n",
    "\n",
    "        \n",
    "def extract_similar_image(image_path, video_path, write_path, encoder, min_similarity, max_similarity, mask=mask):\n",
    "    \"\"\"\n",
    "    For a given image, extract a similar sample from the corresponding video (if it exists) and save it.\n",
    "    \"\"\"\n",
    "    # Extracting and encoding the reference expert image\n",
    "    exp_image = cv2.imread(image_path)\n",
    "    exp_image_code = encoder(exp_image).reshape(-1)\n",
    "    \n",
    "    # Video exploitation\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    last_seen = LAST_SEEN_NB_FRAMES # variable to avoid considering two sample images that are almost the same \n",
    "    extraction_success = False\n",
    "    \n",
    "    while(video.isOpened()):\n",
    "        success, sample_image = video.read()\n",
    "        if (success == True):\n",
    "            if (last_seen >= LAST_SEEN_NB_FRAMES) :\n",
    "                # If we have access to an image, we encode it and verify the similarity to the expert image\n",
    "                sample_image_code = encoder(sample_image).reshape(-1)\n",
    "                similarity = cosine_similarity(exp_image_code, sample_image_code)\n",
    "                # If the similarity criterion is satisfied, the image is selected\n",
    "                if (similarity > min_similarity and similarity < max_similarity):\n",
    "                    \n",
    "                    # Saving sampled image\n",
    "                    _, image_id = os.path.split(image_path)\n",
    "                    cv2.imwrite(f\"{write_path}false/{image_id}\", resize_crop(sample_image, mask))\n",
    "                    \n",
    "                    # Saving corresponding expert image\n",
    "                    cv2.imwrite(f\"{write_path}true/{image_id}\", resize_crop(exp_image, mask))\n",
    "                    \n",
    "                    extraction_success = True\n",
    "                    video.release()\n",
    "                    break # cv2 does not release video fast enough\n",
    "                else:\n",
    "                    last_seen = 0\n",
    "        else:\n",
    "            #need to get out the while loop if we can't read a file\n",
    "            video.release()\n",
    "        last_seen += 1\n",
    "    \n",
    "    return extraction_success\n",
    "\n",
    "        \n",
    "def find_video(image_path, videos_path):\n",
    "    \"\"\"\n",
    "    For a given unique image path, find if there is corresponding video in the given list of videos paths\n",
    "    \n",
    "    image_path: str, path of single image\n",
    "    videos_path: set containing paths of all videos\n",
    "    return: bool, str (operation success, video path)\n",
    "    \"\"\"\n",
    "    video_path = image_path[:-4] + '.mp4'\n",
    "    if(video_path in videos_path):\n",
    "        return True, video_path\n",
    "    else:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not obtain enough samples: \n",
      "Obtained : 199 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 44 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 294 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 111 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 480 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 290 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 552 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 429 \n",
      "Demanded: 1000\n"
     ]
    }
   ],
   "source": [
    "similarity_bounds = [[0.75, 0.85], [0.8, 0.9], [0.85, 0.95], [0.9, 0.97]]\n",
    "cnn = CnnEncoder()\n",
    "cnn = cnn.eval()\n",
    "encoders = [(create_brightness_matrix, 'brightMat'), (cnn, 'cnn')]\n",
    "for bounds in similarity_bounds:\n",
    "    for encoder in encoders:\n",
    "        dataset_path = f'data/butter_proc_images_{encoder[1]}_{bounds[0]}_{bounds[1]}/'\n",
    "        make_directory(dataset_path)\n",
    "        generate_dataset(read_path='data/Ultrason_butterflynetwork/*',\n",
    "                         write_path=dataset_path,\n",
    "                         encoder=encoder[0],\n",
    "                         min_similarity=bounds[0],\n",
    "                         max_similarity=bounds[1],\n",
    "                         nb_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training\n",
    "\n",
    "TODO pytorch tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_data(data_path):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_path, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'val']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                                 shuffle=True, num_workers=4)\n",
    "                  for x in ['train', 'val']}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    \n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "    \n",
    "def get_model_and_parameters(model_type):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    # Loading pretrained model\n",
    "    if(model_type is \"resnet18\"):\n",
    "        model = models.resnet18(pretrained=True)\n",
    "    elif(model_type is \"googlenet\"):\n",
    "        model = models.googlenet(pretrained=True)\n",
    "    else:\n",
    "        raise Error(f\"Model {model_type} is not supported\")\n",
    "        \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    \n",
    "    return model, criterion, optimizer_ft, exp_lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    losses_tr = []\n",
    "    losses_val = []\n",
    "    acc_tr = []\n",
    "    acc_val = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Storing loss and accuracy values\n",
    "            if phase == 'train':\n",
    "                losses_tr.append(epoch_loss)\n",
    "                acc_tr.append(epoch_acc)\n",
    "            else:\n",
    "                losses_val.append(epoch_loss)\n",
    "                acc_val.append(epoch_acc)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}\\n'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, losses_tr, losses_val, acc_tr, acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\n",
      "mkdir: cannot create directory ‘models/similarity’: File exists\n",
      "mkdir: cannot create directory ‘figures’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir models\n",
    "! mkdir models/similarity\n",
    "! mkdir figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ########## Encoder (brightMat) with similarity bounds (0.75) and (0.85) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5384 Acc: 0.7313\n",
      "val Loss: 0.6140 Acc: 0.7179\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.5747 Acc: 0.7938\n",
      "val Loss: 0.6292 Acc: 0.6795\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.5659 Acc: 0.7563\n",
      "val Loss: 0.6798 Acc: 0.6667\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.4811 Acc: 0.8094\n",
      "val Loss: 0.3348 Acc: 0.8846\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.4948 Acc: 0.8188\n",
      "val Loss: 0.4725 Acc: 0.7821\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.7348 Acc: 0.7969\n",
      "val Loss: 1.4744 Acc: 0.5000\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.4915 Acc: 0.8156\n",
      "val Loss: 0.2174 Acc: 0.9103\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1883 Acc: 0.9281\n",
      "val Loss: 0.0798 Acc: 0.9615\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.2888 Acc: 0.8906\n",
      "val Loss: 0.3748 Acc: 0.8077\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1433 Acc: 0.9438\n",
      "val Loss: 0.0704 Acc: 0.9872\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1964 Acc: 0.9188\n",
      "val Loss: 0.0510 Acc: 0.9872\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1599 Acc: 0.9313\n",
      "val Loss: 0.0454 Acc: 0.9872\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1373 Acc: 0.9344\n",
      "val Loss: 0.0493 Acc: 0.9872\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1921 Acc: 0.9156\n",
      "val Loss: 0.0524 Acc: 0.9872\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.2229 Acc: 0.9094\n",
      "val Loss: 0.0734 Acc: 0.9744\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1605 Acc: 0.9375\n",
      "val Loss: 0.0391 Acc: 0.9872\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.2235 Acc: 0.8969\n",
      "val Loss: 0.0814 Acc: 0.9744\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1771 Acc: 0.9219\n",
      "val Loss: 0.0365 Acc: 0.9872\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1645 Acc: 0.9313\n",
      "val Loss: 0.0366 Acc: 0.9872\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.2104 Acc: 0.8969\n",
      "val Loss: 0.0331 Acc: 0.9872\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1536 Acc: 0.9406\n",
      "val Loss: 0.0399 Acc: 0.9872\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.2283 Acc: 0.9156\n",
      "val Loss: 0.0415 Acc: 0.9872\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.2180 Acc: 0.9188\n",
      "val Loss: 0.0369 Acc: 0.9872\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1884 Acc: 0.9188\n",
      "val Loss: 0.0433 Acc: 0.9872\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1230 Acc: 0.9531\n",
      "val Loss: 0.0374 Acc: 0.9872\n",
      "\n",
      "Training complete in 1m 16s\n",
      "Best val Acc: 0.987179\n",
      "\n",
      "Carbon footprint : 2.35 gCO2eq\n",
      "\n",
      " ########## Encoder (cnn) with similarity bounds (0.75) and (0.85) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.6666 Acc: 0.6111\n",
      "val Loss: 0.6723 Acc: 0.6875\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.6239 Acc: 0.6667\n",
      "val Loss: 0.5655 Acc: 0.7500\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.3616 Acc: 0.8194\n",
      "val Loss: 1.0211 Acc: 0.5625\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.6007 Acc: 0.7778\n",
      "val Loss: 0.6742 Acc: 0.6875\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.3925 Acc: 0.8750\n",
      "val Loss: 0.7595 Acc: 0.6875\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.3349 Acc: 0.9028\n",
      "val Loss: 0.6512 Acc: 0.6250\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.3319 Acc: 0.8889\n",
      "val Loss: 0.5200 Acc: 0.6875\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1356 Acc: 0.9444\n",
      "val Loss: 0.0747 Acc: 1.0000\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1909 Acc: 0.9444\n",
      "val Loss: 0.0481 Acc: 1.0000\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.2871 Acc: 0.8889\n",
      "val Loss: 0.0735 Acc: 1.0000\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1930 Acc: 0.9306\n",
      "val Loss: 0.0875 Acc: 1.0000\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.2282 Acc: 0.8750\n",
      "val Loss: 0.0541 Acc: 1.0000\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0745 Acc: 0.9861\n",
      "val Loss: 0.0633 Acc: 1.0000\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1528 Acc: 0.9444\n",
      "val Loss: 0.0252 Acc: 1.0000\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.0927 Acc: 0.9722\n",
      "val Loss: 0.0266 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0454 Acc: 1.0000\n",
      "val Loss: 0.0238 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0637 Acc: 0.9583\n",
      "val Loss: 0.0221 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1114 Acc: 0.9444\n",
      "val Loss: 0.0249 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1226 Acc: 0.9583\n",
      "val Loss: 0.0248 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.9444\n",
      "val Loss: 0.0244 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0709 Acc: 0.9722\n",
      "val Loss: 0.0220 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0631 Acc: 0.9861\n",
      "val Loss: 0.0228 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.2245 Acc: 0.8750\n",
      "val Loss: 0.0267 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0595 Acc: 0.9722\n",
      "val Loss: 0.0223 Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1776 Acc: 0.8889\n",
      "val Loss: 0.0233 Acc: 1.0000\n",
      "\n",
      "Training complete in 0m 24s\n",
      "Best val Acc: 1.000000\n",
      "\n",
      "Carbon footprint : 0.75 gCO2eq\n",
      "\n",
      " ########## Encoder (brightMat) with similarity bounds (0.8) and (0.9) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.4856 Acc: 0.7860\n",
      "val Loss: 1.3040 Acc: 0.6034\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.4514 Acc: 0.8369\n",
      "val Loss: 0.4656 Acc: 0.7759\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.3162 Acc: 0.8835\n",
      "val Loss: 0.3471 Acc: 0.8448\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.3326 Acc: 0.8750\n",
      "val Loss: 4.3745 Acc: 0.5000\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.4686 Acc: 0.8411\n",
      "val Loss: 0.5996 Acc: 0.6983\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.3395 Acc: 0.8581\n",
      "val Loss: 0.6790 Acc: 0.6983\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.2116 Acc: 0.9280\n",
      "val Loss: 0.6925 Acc: 0.6379\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.2237 Acc: 0.9174\n",
      "val Loss: 0.1078 Acc: 0.9741\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.2621 Acc: 0.8877\n",
      "val Loss: 0.0271 Acc: 0.9828\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1801 Acc: 0.9343\n",
      "val Loss: 0.0268 Acc: 0.9828\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1269 Acc: 0.9576\n",
      "val Loss: 0.0435 Acc: 0.9828\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1048 Acc: 0.9682\n",
      "val Loss: 0.0247 Acc: 1.0000\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.2147 Acc: 0.9237\n",
      "val Loss: 0.4400 Acc: 0.7759\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1831 Acc: 0.9195\n",
      "val Loss: 0.0314 Acc: 1.0000\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1453 Acc: 0.9280\n",
      "val Loss: 0.0223 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1816 Acc: 0.9216\n",
      "val Loss: 0.0246 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1627 Acc: 0.9386\n",
      "val Loss: 0.0245 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1381 Acc: 0.9470\n",
      "val Loss: 0.0234 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1099 Acc: 0.9619\n",
      "val Loss: 0.0245 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1981 Acc: 0.9237\n",
      "val Loss: 0.0231 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1444 Acc: 0.9407\n",
      "val Loss: 0.0236 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1771 Acc: 0.9195\n",
      "val Loss: 0.0227 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1317 Acc: 0.9428\n",
      "val Loss: 0.0213 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1077 Acc: 0.9619\n",
      "val Loss: 0.0231 Acc: 0.9914\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1321 Acc: 0.9449\n",
      "val Loss: 0.0256 Acc: 0.9914\n",
      "\n",
      "Training complete in 1m 44s\n",
      "Best val Acc: 1.000000\n",
      "\n",
      "Carbon footprint : 3.23 gCO2eq\n",
      "\n",
      " ########## Encoder (cnn) with similarity bounds (0.8) and (0.9) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5887 Acc: 0.7247\n",
      "val Loss: 0.8758 Acc: 0.5455\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.4583 Acc: 0.7865\n",
      "val Loss: 0.5754 Acc: 0.7500\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.3267 Acc: 0.8708\n",
      "val Loss: 1.1548 Acc: 0.5000\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2216 Acc: 0.9270\n",
      "val Loss: 0.4012 Acc: 0.7955\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.4112 Acc: 0.8596\n",
      "val Loss: 1.5011 Acc: 0.5455\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.3631 Acc: 0.8539\n",
      "val Loss: 0.3247 Acc: 0.8864\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.2564 Acc: 0.8820\n",
      "val Loss: 0.8821 Acc: 0.6591\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.4130 Acc: 0.8371\n",
      "val Loss: 0.0194 Acc: 1.0000\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.2548 Acc: 0.9270\n",
      "val Loss: 0.0161 Acc: 1.0000\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.2056 Acc: 0.9438\n",
      "val Loss: 0.0147 Acc: 1.0000\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0974 Acc: 0.9494\n",
      "val Loss: 0.0110 Acc: 1.0000\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1126 Acc: 0.9494\n",
      "val Loss: 0.0091 Acc: 1.0000\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1706 Acc: 0.9438\n",
      "val Loss: 0.0118 Acc: 1.0000\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1008 Acc: 0.9663\n",
      "val Loss: 0.0132 Acc: 1.0000\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1585 Acc: 0.9157\n",
      "val Loss: 0.0108 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1358 Acc: 0.9494\n",
      "val Loss: 0.0118 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1609 Acc: 0.9326\n",
      "val Loss: 0.0099 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.9831\n",
      "val Loss: 0.0180 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1821 Acc: 0.9213\n",
      "val Loss: 0.0070 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.2173 Acc: 0.8820\n",
      "val Loss: 0.0193 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1618 Acc: 0.9326\n",
      "val Loss: 0.0242 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1387 Acc: 0.9494\n",
      "val Loss: 0.0085 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9270\n",
      "val Loss: 0.0103 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1582 Acc: 0.9326\n",
      "val Loss: 0.0123 Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1621 Acc: 0.9438\n",
      "val Loss: 0.0113 Acc: 1.0000\n",
      "\n",
      "Training complete in 0m 43s\n",
      "Best val Acc: 1.000000\n",
      "\n",
      "Carbon footprint : 1.34 gCO2eq\n",
      "\n",
      " ########## Encoder (brightMat) with similarity bounds (0.85) and (0.95) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5538 Acc: 0.7279\n",
      "val Loss: 1.3514 Acc: 0.5000\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.5332 Acc: 0.7995\n",
      "val Loss: 4.2727 Acc: 0.5000\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.3659 Acc: 0.8568\n",
      "val Loss: 0.4039 Acc: 0.8229\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.3740 Acc: 0.8698\n",
      "val Loss: 3.3421 Acc: 0.5000\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.2785 Acc: 0.9115\n",
      "val Loss: 0.7127 Acc: 0.8281\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.2631 Acc: 0.9049\n",
      "val Loss: 5.2840 Acc: 0.5000\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.3878 Acc: 0.8763\n",
      "val Loss: 6.2396 Acc: 0.5000\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1719 Acc: 0.9388\n",
      "val Loss: 0.0164 Acc: 0.9948\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1751 Acc: 0.9284\n",
      "val Loss: 0.0411 Acc: 0.9844\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1352 Acc: 0.9505\n",
      "val Loss: 0.0115 Acc: 1.0000\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1665 Acc: 0.9375\n",
      "val Loss: 0.0367 Acc: 0.9896\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1679 Acc: 0.9245\n",
      "val Loss: 0.0135 Acc: 1.0000\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1322 Acc: 0.9479\n",
      "val Loss: 0.0159 Acc: 0.9948\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1422 Acc: 0.9336\n",
      "val Loss: 0.0119 Acc: 1.0000\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1148 Acc: 0.9596\n",
      "val Loss: 0.0118 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1322 Acc: 0.9505\n",
      "val Loss: 0.0094 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1478 Acc: 0.9375\n",
      "val Loss: 0.0100 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1023 Acc: 0.9570\n",
      "val Loss: 0.0118 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1251 Acc: 0.9518\n",
      "val Loss: 0.0086 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1196 Acc: 0.9466\n",
      "val Loss: 0.0102 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1411 Acc: 0.9401\n",
      "val Loss: 0.0131 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0970 Acc: 0.9635\n",
      "val Loss: 0.0115 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1140 Acc: 0.9622\n",
      "val Loss: 0.0116 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1559 Acc: 0.9323\n",
      "val Loss: 0.0096 Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1710 Acc: 0.9258\n",
      "val Loss: 0.0098 Acc: 1.0000\n",
      "\n",
      "Training complete in 2m 39s\n",
      "Best val Acc: 1.000000\n",
      "\n",
      "Carbon footprint : 4.94 gCO2eq\n",
      "\n",
      " ########## Encoder (cnn) with similarity bounds (0.85) and (0.95) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.6265 Acc: 0.6595\n",
      "val Loss: 0.7687 Acc: 0.6034\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.4053 Acc: 0.8427\n",
      "val Loss: 1.8933 Acc: 0.5086\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.3578 Acc: 0.8513\n",
      "val Loss: 1.1681 Acc: 0.5603\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.3467 Acc: 0.8664\n",
      "val Loss: 1.6421 Acc: 0.5000\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.3391 Acc: 0.8642\n",
      "val Loss: 2.7288 Acc: 0.5000\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.3691 Acc: 0.8513\n",
      "val Loss: 0.4804 Acc: 0.7414\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.2244 Acc: 0.9246\n",
      "val Loss: 0.7485 Acc: 0.6724\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1652 Acc: 0.9397\n",
      "val Loss: 0.0074 Acc: 1.0000\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1724 Acc: 0.9246\n",
      "val Loss: 0.4358 Acc: 0.8362\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1142 Acc: 0.9461\n",
      "val Loss: 0.0112 Acc: 1.0000\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0961 Acc: 0.9526\n",
      "val Loss: 0.0125 Acc: 1.0000\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1064 Acc: 0.9504\n",
      "val Loss: 0.0078 Acc: 1.0000\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1583 Acc: 0.9353\n",
      "val Loss: 0.0065 Acc: 1.0000\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1343 Acc: 0.9504\n",
      "val Loss: 0.0544 Acc: 0.9655\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1321 Acc: 0.9353\n",
      "val Loss: 0.0086 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1124 Acc: 0.9461\n",
      "val Loss: 0.0077 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1376 Acc: 0.9375\n",
      "val Loss: 0.0064 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1260 Acc: 0.9547\n",
      "val Loss: 0.0099 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0812 Acc: 0.9741\n",
      "val Loss: 0.0075 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0558 Acc: 0.9784\n",
      "val Loss: 0.0066 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1520 Acc: 0.9353\n",
      "val Loss: 0.0051 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1232 Acc: 0.9397\n",
      "val Loss: 0.0070 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1246 Acc: 0.9461\n",
      "val Loss: 0.0074 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0598 Acc: 0.9741\n",
      "val Loss: 0.0066 Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1098 Acc: 0.9526\n",
      "val Loss: 0.0052 Acc: 1.0000\n",
      "\n",
      "Training complete in 1m 39s\n",
      "Best val Acc: 1.000000\n",
      "\n",
      "Carbon footprint : 3.08 gCO2eq\n",
      "\n",
      " ########## Encoder (brightMat) with similarity bounds (0.9) and (0.97) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5676 Acc: 0.7658\n",
      "val Loss: 3.3088 Acc: 0.5000\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.4205 Acc: 0.8382\n",
      "val Loss: 0.8945 Acc: 0.5273\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.4161 Acc: 0.8541\n",
      "val Loss: 1.3296 Acc: 0.5864\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.3722 Acc: 0.8688\n",
      "val Loss: 0.4393 Acc: 0.8364\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.2465 Acc: 0.9140\n",
      "val Loss: 5.5039 Acc: 0.5000\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.3735 Acc: 0.8756\n",
      "val Loss: 0.7419 Acc: 0.6636\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.2788 Acc: 0.8959\n",
      "val Loss: 1.7120 Acc: 0.5091\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1709 Acc: 0.9287\n",
      "val Loss: 0.0365 Acc: 0.9955\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1359 Acc: 0.9491\n",
      "val Loss: 0.0866 Acc: 0.9682\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1324 Acc: 0.9480\n",
      "val Loss: 0.0260 Acc: 0.9955\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9480\n",
      "val Loss: 0.0793 Acc: 0.9636\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1159 Acc: 0.9604\n",
      "val Loss: 0.0603 Acc: 0.9864\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1184 Acc: 0.9480\n",
      "val Loss: 0.0231 Acc: 0.9955\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1116 Acc: 0.9514\n",
      "val Loss: 0.0187 Acc: 0.9955\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1094 Acc: 0.9536\n",
      "val Loss: 0.0180 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1263 Acc: 0.9502\n",
      "val Loss: 0.0232 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1375 Acc: 0.9400\n",
      "val Loss: 0.0322 Acc: 0.9864\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1122 Acc: 0.9525\n",
      "val Loss: 0.0331 Acc: 0.9909\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1008 Acc: 0.9649\n",
      "val Loss: 0.0186 Acc: 0.9955\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0970 Acc: 0.9548\n",
      "val Loss: 0.0180 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1501 Acc: 0.9389\n",
      "val Loss: 0.0390 Acc: 0.9864\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1372 Acc: 0.9378\n",
      "val Loss: 0.0211 Acc: 0.9955\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1175 Acc: 0.9514\n",
      "val Loss: 0.0197 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1065 Acc: 0.9548\n",
      "val Loss: 0.0315 Acc: 0.9955\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1118 Acc: 0.9491\n",
      "val Loss: 0.0268 Acc: 1.0000\n",
      "\n",
      "Training complete in 3m 2s\n",
      "Best val Acc: 1.000000\n",
      "\n",
      "Carbon footprint : 5.64 gCO2eq\n",
      "\n",
      " ########## Encoder (cnn) with similarity bounds (0.9) and (0.97) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5265 Acc: 0.7500\n",
      "val Loss: 2.4220 Acc: 0.5000\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.7256 Acc: 0.7660\n",
      "val Loss: 0.6891 Acc: 0.7353\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.4059 Acc: 0.8445\n",
      "val Loss: 0.3886 Acc: 0.8176\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.4476 Acc: 0.8503\n",
      "val Loss: 0.7059 Acc: 0.6588\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.3630 Acc: 0.8706\n",
      "val Loss: 1.2595 Acc: 0.6824\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.2942 Acc: 0.9113\n",
      "val Loss: 11.2322 Acc: 0.5000\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.2857 Acc: 0.9113\n",
      "val Loss: 2.9592 Acc: 0.5176\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1813 Acc: 0.9273\n",
      "val Loss: 0.0888 Acc: 0.9588\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.9390\n",
      "val Loss: 0.0383 Acc: 0.9765\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1764 Acc: 0.9244\n",
      "val Loss: 0.3173 Acc: 0.8529\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1329 Acc: 0.9317\n",
      "val Loss: 0.0331 Acc: 0.9941\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1186 Acc: 0.9535\n",
      "val Loss: 0.0197 Acc: 0.9941\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1609 Acc: 0.9273\n",
      "val Loss: 0.0316 Acc: 0.9941\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1215 Acc: 0.9520\n",
      "val Loss: 0.0703 Acc: 0.9824\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1628 Acc: 0.9288\n",
      "val Loss: 0.0280 Acc: 0.9824\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1023 Acc: 0.9520\n",
      "val Loss: 0.0309 Acc: 0.9824\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1340 Acc: 0.9390\n",
      "val Loss: 0.0238 Acc: 0.9941\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1195 Acc: 0.9433\n",
      "val Loss: 0.0305 Acc: 0.9882\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.9462\n",
      "val Loss: 0.0256 Acc: 0.9941\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1108 Acc: 0.9506\n",
      "val Loss: 0.0299 Acc: 0.9941\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1074 Acc: 0.9564\n",
      "val Loss: 0.0263 Acc: 0.9941\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0823 Acc: 0.9724\n",
      "val Loss: 0.0236 Acc: 0.9941\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1093 Acc: 0.9549\n",
      "val Loss: 0.0233 Acc: 0.9941\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.9578\n",
      "val Loss: 0.0271 Acc: 0.9941\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1049 Acc: 0.9535\n",
      "val Loss: 0.0249 Acc: 0.9941\n",
      "\n",
      "Training complete in 2m 26s\n",
      "Best val Acc: 0.994118\n",
      "\n",
      "Carbon footprint : 4.53 gCO2eq\n"
     ]
    }
   ],
   "source": [
    "model_type = \"resnet18\"\n",
    "\n",
    "for bounds in similarity_bounds:\n",
    "    for encoder in encoders:\n",
    "        dataset_path = f'data/butter_proc_images_{encoder[1]}_{bounds[0]}_{bounds[1]}/'\n",
    "        dataloaders, dataset_sizes, class_names = get_data(dataset_path)\n",
    "        model, criterion, optimizer_ft, exp_lr_scheduler = get_model_and_parameters(model_type)\n",
    "        \n",
    "        print(f'\\n ########## Encoder ({encoder[1]}) with similarity bounds ({bounds[0]}) and ({bounds[1]}) ##########\\n')\n",
    "        \n",
    "        # Carbon footprint measurement\n",
    "        cumulator = base.Cumulator()\n",
    "        cumulator.on()\n",
    "        \n",
    "        # Training model\n",
    "        model_trained, losses_tr, losses_val, acc_tr, acc_val = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                                    num_epochs=25)\n",
    "        cumulator.off()\n",
    "        print(f\"Carbon footprint : {cumulator.total_carbon_footprint():.2f} gCO2eq\")\n",
    "        \n",
    "        # Saving model\n",
    "        torch.save(model.state_dict(), f\"models/similarity/{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}\")\n",
    "        \n",
    "        # Plotting\n",
    "        plot(losses_tr, losses_val, 'Training loss', 'Validation loss',\n",
    "             f'Losses {bounds[0]}_{bounds[1]}', \n",
    "             f'figures/losses_{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}.png', 'loss')\n",
    "        plot(acc_tr, acc_val, 'Training accuracy', 'Validation accuracy',\n",
    "             f'Accuracy {bounds[0]}_{bounds[1]}',\n",
    "             f'figures/accuracy_{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}.png', 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model testing\n",
    "\n",
    "With our data correctly labeled and organised, we can now proceed to training our model. Training is done in the ```classifier.ipynb``` notebook. In this notebook we directly load our trained models and test them.\n",
    "\n",
    "### 4.1 Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(load_path, device, model_type):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    # Loading model architecture\n",
    "    if(model_type is \"resnet18\"):\n",
    "        model = models.resnet18(pretrained=True)\n",
    "    elif(model_type is \"googlenet\"):\n",
    "        model = models.googlenet(pretrained=True)\n",
    "    else:\n",
    "        raise Error(f\"Model {model_type} is not supported\")\n",
    "        \n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "    # Loading parameters to the model\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Testing\n",
    "\n",
    "In order to test our model's extraction power we once again use the notion of similarity. For our selected test images, we iterate through the corresponding video running each frame into our model. The one frame that achieves highest confidence as a \"True\" image is to be selected. We then compute the similarity between our selected image and the expert one. This measure is our testing metric. The higher, the closer our model as been able to emulate the expert's decision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_model(img):\n",
    "    \"\"\"\n",
    "    Read image and apply required transformations for model usage\n",
    "    \"\"\"\n",
    "    # Read, resize and crop\n",
    "    img = resize_crop(img, mask)\n",
    "    \n",
    "    # Transform to PIL format\n",
    "    img = Image.fromarray(img)\n",
    "    \n",
    "    # Images transformations to apply before entering model\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    return transform(img)\n",
    "\n",
    "\n",
    "def test_model(model, test_folder_path, encoder):\n",
    "    \"\"\"\n",
    "    Test the given model on images and videos contained in the given folder, based on a similarity metric\n",
    "    Return the average similarity between expert images and model selectd images from the corresponding videos\n",
    "    \"\"\"\n",
    "    images_path = test_folder_path + 'test/*.png'\n",
    "    images_path = glob.glob(images_path)\n",
    "    \n",
    "    videos_path = test_folder_path + 'test/*.mp4'\n",
    "    videos_path = set(glob.glob(videos_path)) # transformed to set for faster queries\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    # Use model on image every 'skip' frames\n",
    "    skip = 10\n",
    "    # TODO debug stuff\n",
    "    #images_path = images_path[:3]\n",
    "    #print(images_path)\n",
    "    \n",
    "    for image_path in images_path:\n",
    "        # Finding corresponding video\n",
    "        success, video_path = find_video(image_path, videos_path)\n",
    "        if(success == True):\n",
    "            # Scanning the video\n",
    "            video = cv2.VideoCapture(video_path)\n",
    "            frame = 0\n",
    "            best_confidence = 0\n",
    "            selected_image = None\n",
    "            while(video.isOpened()):\n",
    "                success, sample_image = video.read()\n",
    "                if(success == True):\n",
    "                    frame += 1\n",
    "                    if(frame % skip == 0):\n",
    "                        # Sampled image goes through the model\n",
    "                        model_image = prepare_for_model(sample_image)\n",
    "                        model_image = torch.unsqueeze(model_image, 0) # adding dummy batch dimension\n",
    "                        model_image = model_image.to(device)\n",
    "                        probabilities = nn.functional.softmax(model(model_image), dim = 1)\n",
    "                        confidence = probabilities[0,1]\n",
    "\n",
    "                        if(confidence >= best_confidence):\n",
    "                            best_confidence = confidence\n",
    "                            selected_image = sample_image.copy()\n",
    "                else:\n",
    "                    video.release()\n",
    "            \n",
    "            # Computing similarity\n",
    "            if selected_image is not None:\n",
    "                exp_image = cv2.imread(image_path)\n",
    "                exp_image_code = encoder(exp_image).reshape(-1)\n",
    "                selected_image_code = encoder(selected_image).reshape(-1)\n",
    "                similarity = cosine_similarity(exp_image_code, selected_image_code)\n",
    "                similarities.append(similarity)\n",
    "    \n",
    "    # Averaging similarity TODO think of another method ?\n",
    "    return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing average similarity over the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.75) - (0.85), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9391\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.75) - (0.85), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9401\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.75) - (0.85), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9724\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.75) - (0.85), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9764\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.8) - (0.9), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9380\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.8) - (0.9), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9341\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.8) - (0.9), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9714\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.8) - (0.9), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9685\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.85) - (0.95), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9354\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.85) - (0.95), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9394\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.85) - (0.95), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9665\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.85) - (0.95), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9729\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.9) - (0.97), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9325\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.9) - (0.97), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9259\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.9) - (0.97), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9731\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.9) - (0.97), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9698\n"
     ]
    }
   ],
   "source": [
    "model_type = \"resnet18\"\n",
    "test_folder_path = \"data/test/\"\n",
    "\n",
    "for bounds in similarity_bounds:\n",
    "    for test_encoder in encoders:\n",
    "        for encoder in encoders:\n",
    "            # Loading model\n",
    "            model_path = f'models/similarity/{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}'\n",
    "            model = load_model(model_path, device, model_type)\n",
    "            print(f'\\n ########## Model trained with encoder ({encoder[1]}) and similarity bounds ({bounds[0]}) - ({bounds[1]}), test similarity through {test_encoder[1]} ##########\\n')\n",
    "\n",
    "            # Testing\n",
    "            mean_similarity = test_model(model, test_folder_path, test_encoder[0])\n",
    "            print(f'Mean similarity on test set: {mean_similarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confident_images(model, test_folder_path, write_path):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"   \n",
    "    videos_path = test_folder_path + 'test/*.mp4'\n",
    "    videos_path = glob.glob(videos_path)\n",
    "    \n",
    "    good_counter = 0\n",
    "    avg_counter = 0\n",
    "    bad_counter = 0\n",
    "    \n",
    "    # Use model on image every 'skip' frames\n",
    "    skip = 1\n",
    "    # TODO debug stuff\n",
    "    #images_path = images_path[:3]\n",
    "    #print(images_path)\n",
    "    \n",
    "    for video_path in videos_path:\n",
    "        # Scanning the video\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        frame = 0\n",
    "        best_confidence = 0\n",
    "        min_best_confidence = 0.7\n",
    "        selected_image = None\n",
    "        average_image = None\n",
    "        bad_image = None\n",
    "        while(video.isOpened()):\n",
    "            success, sample_image = video.read()\n",
    "            if(success == True):\n",
    "                frame += 1\n",
    "                if(frame % skip == 0):\n",
    "                    # Sampled image goes through the model\n",
    "                    model_image = prepare_for_model(sample_image)\n",
    "                    model_image = torch.unsqueeze(model_image, 0) # adding dummy batch dimension\n",
    "                    model_image = model_image.to(device)\n",
    "                    probabilities = nn.functional.softmax(model(model_image), dim = 1)\n",
    "                    confidence = probabilities[0,1]\n",
    "                    if((confidence >= min_best_confidence) and (confidence >= best_confidence)):\n",
    "                        best_confidence = confidence\n",
    "                        selected_image = sample_image.copy()\n",
    "                    \n",
    "                    if((average_image is None) and ((confidence < 0.7) and (confidence > 0.2))):\n",
    "                        average_image = sample_image.copy()\n",
    "                        \n",
    "                    if((bad_image is None) and (confidence < 0.2)):\n",
    "                        bad_image = sample_image.copy()                         \n",
    "            else:\n",
    "                video.release()\n",
    "\n",
    "        # Writing images\n",
    "        image_id = os.path.split(video_path)[-1][:-4] + '.png'\n",
    "        if selected_image is not None:\n",
    "            good_counter +=1\n",
    "            cv2.imwrite(f\"{write_path}best/{image_id}\", resize_crop(selected_image, mask))\n",
    "        \n",
    "        if average_image is not None:\n",
    "            avg_counter +=1\n",
    "            cv2.imwrite(f\"{write_path}average/{image_id}\", resize_crop(average_image, mask))\n",
    "        \n",
    "        if bad_image is not None:\n",
    "            bad_counter +=1\n",
    "            cv2.imwrite(f\"{write_path}bad/{image_id}\", resize_crop(bad_image, mask))\n",
    "            \n",
    "    print(\"Good\", good_counter)\n",
    "    print(\"Average\", avg_counter)\n",
    "    print(\"Bad\", bad_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/selected_images’: File exists\n",
      "mkdir: cannot create directory ‘data/selected_images/best’: File exists\n",
      "mkdir: cannot create directory ‘data/selected_images/average’: File exists\n",
      "mkdir: cannot create directory ‘data/selected_images/bad’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir data/selected_images\n",
    "! mkdir data/selected_images/best\n",
    "! mkdir data/selected_images/average\n",
    "! mkdir data/selected_images/bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good 52\n",
      "Average 65\n",
      "Bad 119\n"
     ]
    }
   ],
   "source": [
    "model_path = f'models/similarity/resnet18_natural_cnn_0.8_0.9'\n",
    "model = load_model(model_path, device, model_type)\n",
    "generate_confident_images(model, test_folder_path, 'data/selected_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.75) - (0.85) ##########\n",
      "\n",
      "Good 35\n",
      "Ugly 89\n",
      "Bad 131\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.75) - (0.85) ##########\n",
      "\n",
      "Good 96\n",
      "Ugly 100\n",
      "Bad 74\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.8) - (0.9) ##########\n",
      "\n",
      "Good 0\n",
      "Ugly 11\n",
      "Bad 131\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.8) - (0.9) ##########\n",
      "\n",
      "Good 52\n",
      "Ugly 65\n",
      "Bad 119\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.85) - (0.95) ##########\n",
      "\n",
      "Good 1\n",
      "Ugly 7\n",
      "Bad 131\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.85) - (0.95) ##########\n",
      "\n",
      "Good 3\n",
      "Ugly 36\n",
      "Bad 131\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.9) - (0.97) ##########\n",
      "\n",
      "Good 0\n",
      "Ugly 12\n",
      "Bad 131\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.9) - (0.97) ##########\n",
      "\n",
      "Good 0\n",
      "Ugly 10\n",
      "Bad 131\n"
     ]
    }
   ],
   "source": [
    "for bounds in similarity_bounds:\n",
    "    for encoder in encoders:\n",
    "        # Loading model\n",
    "        model_path = f'models/similarity/{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}'\n",
    "        model = load_model(model_path, device, model_type)\n",
    "        print(f'\\n ########## Model trained with encoder ({encoder[1]}) and similarity bounds ({bounds[0]}) - ({bounds[1]}) ##########\\n')\n",
    "        generate_confident_images(model, test_folder_path, 'data/selected_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_confidence_test_set_images(model, test_folder_path):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"   \n",
    "    images_path = test_folder_path + 'test/*.png'\n",
    "    images_path = glob.glob(images_path)\n",
    "    \n",
    "    \n",
    "    # Use model on image every 'skip' frames\n",
    "    skip = 10\n",
    "    # TODO debug stuff\n",
    "    #images_path = images_path[:3]\n",
    "    #print(images_path)\n",
    "    \n",
    "    good_counter = 0\n",
    "    avg_counter = 0\n",
    "    bad_counter = 0\n",
    "    \n",
    "    for image_path in images_path:\n",
    "        sample_image = cv2.imread(image_path)\n",
    "        # Sampled image goes through the model\n",
    "        model_image = prepare_for_model(sample_image)\n",
    "        model_image = torch.unsqueeze(model_image, 0) # adding dummy batch dimension\n",
    "        model_image = model_image.to(device)\n",
    "        probabilities = nn.functional.softmax(model(model_image), dim = 1)\n",
    "        confidence = probabilities[0,1]\n",
    "        \n",
    "        if confidence > 0.7:\n",
    "            good_counter += 1\n",
    "        elif confidence < 0.2:\n",
    "            bad_counter +=1\n",
    "        else:\n",
    "            avg_counter += 1\n",
    "            \n",
    "    print(\"Good\", good_counter)\n",
    "    print(\"Average\", avg_counter)\n",
    "    print(\"Bad\", bad_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good 154\n",
      "Ugly 3\n",
      "Bad 0\n"
     ]
    }
   ],
   "source": [
    "model_path = f'models/similarity/resnet18_natural_brightMat_0.85_0.95'\n",
    "model = load_model(model_path, device, model_type)\n",
    "give_confidence_test_set_images(model, test_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good 154\n",
      "Average 3\n",
      "Bad 0\n"
     ]
    }
   ],
   "source": [
    "model_path = f'models/similarity/resnet18_natural_cnn_0.75_0.85'\n",
    "model = load_model(model_path, device, model_type)\n",
    "give_confidence_test_set_images(model, test_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good 140\n",
      "Average 12\n",
      "Bad 5\n"
     ]
    }
   ],
   "source": [
    "model_path = f'models/similarity/resnet18_natural_cnn_0.8_0.9'\n",
    "model = load_model(model_path, device, model_type)\n",
    "give_confidence_test_set_images(model, test_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6678709175966463"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CnnEncoder()\n",
    "cnn = cnn.eval()\n",
    "img1 = cnn(cv2.imread('data/Ultrason_butterflynetwork/1.21_QLD.png')).reshape(-1)\n",
    "\n",
    "img2 = (np.random.rand(len(img1)) * 256).reshape(-1)\n",
    "cosine_similarity(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_test = cv2.imread('data/test/test/1.16_QAID.png')\n",
    "image_test = resize_crop(image_test, mask)\n",
    "cv2.imwrite('image_test.png', image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2566080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.012183531004605083"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = cv2.imread('data/test/test/1.16_QAID.png')\n",
    "img1.resize(1080,792,3)\n",
    "print(len(img1.flatten()))\n",
    "cosine_similarity(img1[500,200:210,0].reshape(-1), image_test[500,200:210,0].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 53, 50, 45, 40, 35, 31, 27, 23, 21], dtype=uint8)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = cv2.imread('data/test/test/1.16_QAID.png')\n",
    "img1[500,200:210,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 54, 51, 46, 41, 36, 32, 28, 24, 22], dtype=uint8)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_test[500,200:210,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 791, 3)\n",
      "[62 67 71 73 73 73 72 70 69 68]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "133952272"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = cv2.imread('data/test/test/1.16_QAID.png')\n",
    "print(img1.shape)\n",
    "print(img1[400,200:210,0])\n",
    "img1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 792, 3)\n",
      "[76 67 60 55 52 50 48 47 47 47]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "133952272"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.resize(1080,792,3)\n",
    "print(img1.shape)\n",
    "print(img1[400,200:210,0])\n",
    "img1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lus.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

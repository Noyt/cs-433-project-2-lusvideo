{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YapYOP_srblT"
   },
   "source": [
    "# LUS-CS433\n",
    "\n",
    "TODO general project description + do a readme\n",
    "\n",
    "For this task, we decide to explore a supervised learning approach, using CNNs.\n",
    "Our initial idea is the following: to train a binary classifier ideally able to emulate a clinician's judgement, namely telling us wether an image taken from an LUS video of a patient site should be selected or not.\n",
    "\n",
    "TODO explain why classifier is multi site, wether we should do multiple classifiers (one per site)\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZoIvD9nJuD1"
   },
   "outputs": [],
   "source": [
    "from encoders import CnnEncoder\n",
    "from cumulator import base\n",
    "from plots import plot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBJ5-wpVMirO"
   },
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "#### Image shape\n",
    "We first check whether images in the dataset have all the same shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butterfly_img_path = glob.glob(\"data/Ultrason_butterflynetwork/*.png\")\n",
    "vid_img_path = glob.glob(\"data/Ultrason_butterflynetwork/*.mp4\")\n",
    "\n",
    "def count_formats(path_list):\n",
    "    counts = {}\n",
    "    for path in path_list:\n",
    "        key = cv2.imread(path).shape\n",
    "        if key not in counts.keys():\n",
    "            counts[key] = 0\n",
    "        else:\n",
    "            counts[key] = counts[key] + 1\n",
    "    return counts\n",
    "\n",
    "def count_formats_vid(path_list):\n",
    "    counts = {}\n",
    "    for path in path_list:\n",
    "        video = cv2.VideoCapture(path)\n",
    "        success, img = video.read()\n",
    "        \n",
    "        if success == False :\n",
    "            raise(\"Exception nulle\")\n",
    "        \n",
    "        key = img.shape\n",
    "        video.release()\n",
    "        if key not in counts.keys():\n",
    "            counts[key] = 0\n",
    "        else:\n",
    "            counts[key] = counts[key] + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = cv2.imread('data/Ultrason_butterflynetwork/1.4_QASD_1.png')\n",
    "img = resize_crop(lol, mask, 792, 1080)\n",
    "cv2.imwrite('try_See.png',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butterfly_format = count_formats(butterfly_img_path)\n",
    "butterfly_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_format = count_formats_vid(vid_img_path)\n",
    "vid_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is not everywhere the case, so we can't infer that we'll always the same type of images. It seems that our videos have all the same shapes. However most of our images in the butterfly dataset come from videos. So there may be some videos that have different formats in them? \n",
    "\n",
    "From this analysis, we also get that we have 3447 images in the butterfly dataset, and 1265 videos. \n",
    "\n",
    "Let's inspect more precisely our videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_formats_one_vid(vid_path):\n",
    "    counts = {}\n",
    "    video = cv2.VideoCapture(vid_path)\n",
    "    while(video.isOpened()):\n",
    "        success, img = video.read()\n",
    "        \n",
    "        if success == False :\n",
    "            video.release()\n",
    "            break\n",
    "        key = img.shape\n",
    "        if key not in counts.keys():\n",
    "            counts[key] = 0\n",
    "        else:\n",
    "            counts[key] = counts[key] + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_vid_path = vid_img_path[0]\n",
    "count_formats_one_vid(one_vid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to not be the case. We don't really know where (in most of the cases) this 1 column of pixels went when people saved those images..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is our dataset distributed according to sites?\n",
    "When a clinician performs a lung ultrasound (LUS) examination, he generally acquires several images on different thoracic sites. Each lung is partitioned in four quadrants (anterior or posterior and superior or inferior). They also perform one lung ultrasound on the side of the lung which implies that a patient could have up to 10 different videos of his lungs.\n",
    "\n",
    "Here we look how is our data distributed according to each site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sites(path):\n",
    "    site_nb = {}\n",
    "    for data_path in path:\n",
    "        site_ext = data_path.split('_')[2]#get the site, but could contain extension\n",
    "        site = site_ext.split(\".\")[0]#removes the extension\n",
    "        \n",
    "        if site not in site_nb.keys():\n",
    "            site_nb[site] = 1\n",
    "        else:\n",
    "            site_nb[site] = site_nb[site] + 1\n",
    "    return site_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some files had misspellings, we take fix this here by modifying their names:\n",
    "old_path = 'data/Ultrason_butterflynetwork/1.46_QAiG_1.png'\n",
    "new_path = old_path.replace(\"QAiG\", \"QAIG\", 1)\n",
    "print(new_path)\n",
    "os.rename(old_path, new_path)\n",
    "\n",
    "old_path2 = 'data/Ultrason_butterflynetwork/1.156_QASD-1.png'\n",
    "new_path2 = old_path2.replace(\"QASD-1\", \"QASD_1\")\n",
    "print(new_path2)\n",
    "os.rename(old_path2, new_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butterfly_img_path = glob.glob(\"data/Ultrason_butterflynetwork/*.png\")\n",
    "butterfly_site_nb = dict(sorted(get_sites(butterfly_img_path).items()))\n",
    "butterfly_site_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide that UNK (don't know what it refers to) and QASP (Antérieur Supérieur Postérieur?) images are outliers so we don't consider them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    butterfly_site_nb.pop('UNK')\n",
    "    butterfly_site_nb.pop('QASP')\n",
    "except:\n",
    "    print('It seems they were already removed')\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(butterfly_site_nb.keys(), butterfly_site_nb.values())\n",
    "plt.title('Nb images per site (Butterflynetwork)')\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.xlabel('Sites')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is more or less uniformly distributed, we have around 350-400 images per site. There are just the QLD and QLG sites that have around 220 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same problem here as before:\n",
    "old_path = 'data/Ultrason_butterflynetwork/1.195_QlG.mp4'\n",
    "new_path = old_path.replace(\"QlG\", \"QLG\", 1)\n",
    "print(new_path)\n",
    "os.rename(old_path, new_path)\n",
    "\n",
    "old_path2 = 'data/Ultrason_butterflynetwork/1.184_QAiD_1.mp4'\n",
    "new_path2 = old_path2.replace(\"QAiD\", \"QAID\")\n",
    "print(new_path2)\n",
    "os.rename(old_path2, new_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_img_path = glob.glob(\"data/Ultrason_butterflynetwork/*.mp4\")\n",
    "butterfly_video_site_nb = dict(sorted(get_sites(vid_img_path).items()))\n",
    "butterfly_video_site_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same thing here, we remove UNK videos and QPAG videos as their number is to low.\n",
    "try:\n",
    "    butterfly_video_site_nb.pop('UNK')\n",
    "    butterfly_video_site_nb.pop('QPAG')\n",
    "except:\n",
    "    print('It seems they were already removed')\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(butterfly_video_site_nb.keys(), butterfly_video_site_nb.values())\n",
    "plt.title('Nb videos per site (Butterflynetwork)')\n",
    "plt.ylabel(\"Number of videos\")\n",
    "plt.xlabel('Sites')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, we have a lot more QLD and QLG videos. We know that experts didn't extract an image from every videos or could extract multiple from one. That's why we have this different between the images distribtution and the video distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many patients and videos per patient do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patients_videos(vid_path_list):\n",
    "    \"\"\"\n",
    "    vid_path_list: list of paths of videos\n",
    "    Returns: Dictionnary with patient number (name) and number of videos associated to him \n",
    "    \"\"\"\n",
    "    patients = {}\n",
    "    for path in vid_path_list:\n",
    "        splitted_path = path.split('_') #split the path\n",
    "        patient = splitted_path[0][-1] + \"_\" + splitted_path[1] #extract the patient's number\n",
    "        if patient not in patients.keys():\n",
    "            patients[patient] = 1\n",
    "        else:\n",
    "            patients[patient] = patients[patient] + 1\n",
    "    return patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_patients_videos = patients_videos(vid_img_path)\n",
    "print(f\"We have {len(nb_patients_videos.keys())} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(nb_patients_videos.values(), bins = max(nb_patients_videos.values()))\n",
    "plt.title('Number of patients with x videos dedicated to them')\n",
    "plt.xlabel('Number of videos')\n",
    "plt.ylabel('Number of patients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing this, we want to know how many patients have actually videos from each site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sites_per_patient_videos(vid_path_list):\n",
    "    \"\"\"\n",
    "    vid_path_list: list of paths of videos\n",
    "    Returns: Dictionnary with patient number (name) and all sites that were performed on him\n",
    "    \"\"\"\n",
    "    patients = {}\n",
    "    for path in vid_path_list:\n",
    "        splitted_path = path.split('_') #get the patient and the\n",
    "        patient = splitted_path[0][-1] + \"_\" + splitted_path[1]\n",
    "        \n",
    "        site = splitted_path[2] #get the name of the video\n",
    "        site = site.split(\".\")[0] #removes the extension\n",
    "        \n",
    "        if patient not in patients.keys():\n",
    "            patients[patient] = set([site])\n",
    "        else:\n",
    "            patients[patient].add(site)\n",
    "    return patients\n",
    "\n",
    "sites_per_patient_dic = sites_per_patient_videos(vid_img_path)\n",
    "nb_sites_per_patient = { k : len(v) for k,v in sites_per_patient_dic.items()}\n",
    "\n",
    "plt.hist(nb_sites_per_patient.values(), bins = max(nb_sites_per_patient.values()))\n",
    "plt.title('Number of patients having x sites available on video')\n",
    "plt.xlabel('Number of sites')\n",
    "plt.ylabel('Number of patients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost no patient has a video for each site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5jpXz_R_cP8"
   },
   "source": [
    "## 2. Creating Datasets\n",
    "\n",
    "In order to train our classifier, we require an image dataset with two different kind of labels:\n",
    "\n",
    "* \"True\" images: selected by expert clinicians, these images are known to well represent a patient site and its useful features (such as pleural line, A-lines, B-lines ... etc).\n",
    "* \"False\" images: taken from an LUS video, these images were not selected by an expert clinicians thus they do not represent the perfect shot for a given patient site (image can be blurry, features could be missing or angle shot can be weird and thus image is difficult to interpret).\n",
    "\n",
    "Our initial data contains roughly a thousand expert selected images, which intuitively form our \"True\" labeled images. However, we do not have any image that would correspond to our \"False\" label. Therefore we have to generate them ourselves. We use the following methodology to accomplish our goal:\n",
    "\n",
    "For a given \"True\" image, we iterate through images from its corresponding video. For each image, we compute a similarity value to the expert image and select the image as a False-labelled one based on this criteria. We require that the expert image and the selected image are not too similar to each other (since it would make the classification too hard), yet they should not be too different either (in order to better simulate the clinician options when selecting the best image). The details of the similarity metric are explained in later sections. \n",
    "\n",
    "\n",
    "### 2.1 Creating file folders\n",
    "\n",
    "First of all, we need to define a function that creates folders that will contain our data. These follow a specific architecture that enables us to use PyTorch Datasets and Dataloaders classes conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAq_xxIcQAHU"
   },
   "outputs": [],
   "source": [
    "def make_directory(path):\n",
    "    \"\"\"\n",
    "    Create PyTorch compatible folder architecture\n",
    "    \"\"\"\n",
    "    ! mkdir $path\n",
    "    for kind in ['train/', 'val/']:\n",
    "        sub_path = path + kind\n",
    "        ! mkdir $sub_path\n",
    "        for label in ['true', 'false']:\n",
    "            full_path = sub_path + label\n",
    "            ! mkdir $full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Image preprocessing\n",
    "\n",
    "We start by creating a mask that we will use to crop images, getting rid of features on the images (such as the scale, or name of site) that could otherwise perturb the training.\n",
    "The mask has been crafted manually, as we did not have time to implement automatic edge detrection or similar mechanism to automate the process. We also define a couple functions for common preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main mask used to capture the relevant portion of LUS images. Crafted manually. Is [1,1,1] where image is relevant\n",
    "\n",
    "# Image dimensions\n",
    "nb_rows = 1080\n",
    "nb_cols = 792\n",
    "nb_channels = 3\n",
    "\n",
    "mask = np.zeros([nb_rows, nb_cols, nb_channels])\n",
    "\n",
    "# Filling mask\n",
    "for row in range(nb_rows):\n",
    "    for col in range(nb_cols):\n",
    "        # Delimitations of the cone like portion of a LUS image\n",
    "        if row > 25 and row < 1010 and col < 762 and (-4/5 * row + 293) < col and (4/5*row) + nb_cols-293 > col:\n",
    "            mask[row, col] = [1,1,1]\n",
    "\n",
    "mask = mask.astype('uint8')\n",
    "            \n",
    "            \n",
    "def resize_crop(image, mask, nb_cols=792, nb_rows=1080):\n",
    "    \"\"\"\n",
    "    Resize image and apply mask\n",
    "    \"\"\"\n",
    "    masked_img = cv2.resize(image, (nb_cols, nb_rows))*mask\n",
    "    return masked_img\n",
    "\n",
    "\n",
    "def read_crop(image_path, mask, nb_cols=792, nb_rows=1080):\n",
    "    \"\"\"\n",
    "    Read image, resize to given dimensions and apply mask.\n",
    "    \n",
    "    Returns: image name, image\n",
    "    \"\"\"\n",
    "    # Reading and masking image\n",
    "    cv2_img = cv2.imread(image_path)\n",
    "    masked_img = resize_crop(cv2_img, mask, nb_cols, nb_rows)\n",
    "\n",
    "    # Selecting image name from path name\n",
    "    img_name = os.path.split(image_path)[-1]\n",
    "    \n",
    "    return img_name, masked_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Obtaining Test images\n",
    "\n",
    "Before any dataset generation, we need to select and exclude a few images and video for our testing purposes. We selected ten patient ids by hand. These patients present the advantage of having images and corresponding videos for most of the sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rewrite simply\n",
    "# Create test data folder\n",
    "\n",
    "! mkdir data/test\n",
    "! mkdir data/test/test\n",
    "\n",
    "# Temporary text file used for copying data purposes\n",
    "! > test_data.txt\n",
    "\n",
    "# patient ids used for the test set\n",
    "test_ids = ['.151_','.136_', '.117_', '.45_', '.16_', '.77_', '.164_', '.57', '.36_', '.157_']\n",
    "\n",
    "files_path = glob.glob(\"data/Ultrason_butterflynetwork/*\")\n",
    "test_files = [file for file in files_path if any([(id_ in file) for id_ in test_ids])]\n",
    "\n",
    "file = open(\"test_data.txt\", \"w\")\n",
    "for test_file in test_files:\n",
    "    file.write(test_file+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for file in $(cat test_data.txt); \n",
    "do\n",
    "cp $file data/test/test/\n",
    "done\n",
    "rm test_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Obtaining Training and Validation images\n",
    "\n",
    "We now proceed to dataset generation. First let us define our image similarity mechanism. \n",
    "\n",
    "#### 2.2.1 Similarity between two images\n",
    "\n",
    "The similarity measure was inspired to us by ([Yang et al.](https://arxiv.org/abs/1706.04737), 2017). The idea is to encode a images to lesser dimensional format, reshape them as vector, and finally compute their cosine similarity. The encoding process is necessary to reduce complexity and also avoid potential harm caused by the curse of dimensionality (TODO Prove this). Encoding can be done in various ways. In our project, we selected two methods:\n",
    "* Computing the mean brightness for a couple subparts of the image, in a convolutional fashion.\n",
    "* Using a pretrained CNN model (resnet18) stripped from its last layers, as an encoder\n",
    "\n",
    "The function ```create_brightness_matrix``` implements the first method TODO\n",
    "Second method TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzgnfG9xH3He"
   },
   "outputs": [],
   "source": [
    "def create_brightness_matrix(img, mask=mask, side_length=200):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    img = resize_crop(img, mask)\n",
    "    # half_side determines the number of pixels per step of the \"convolution\"\n",
    "    half_side = int(side_length / 2)\n",
    "    nb_rows = img.shape[0] // half_side\n",
    "    nb_cols = img.shape[1] // half_side\n",
    "    matrix = np.zeros((nb_rows, nb_cols))\n",
    "    \n",
    "    for i in range(nb_rows) :\n",
    "        for j in range(nb_cols) :\n",
    "            matrix[i, j] = np.mean(img[half_side * i : half_side * i + side_length, half_side * j : half_side * j + side_length]) / 3\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors of the same size\n",
    "    \"\"\"\n",
    "    return np.dot(vector1,vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(butterfly_img_path[0])\n",
    "create_brightness_matrix(img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Generating datasets\n",
    "\n",
    "Armed with an encoder and a similarity measure, we can now tackle the image selection problem. We will proceed as described in Section 2.\n",
    "\n",
    "The function ```generate_datasets``` iterates through images present in the given folder, namely the Ultrason butterflynetwork one. For each of these expert selected image, the function tries to find a corresponding (same patient, same site) video through the function ```find_video```. If such video is found, the ```extract_similar_image``` function then iterates through the video, computing the similarity measure between the video's frames and the expert image, in a sequential fashion. If a given frame satisfies the similarity criterias (not too different, not too similar), then both the expert image and the selected frames are saved in their respective folder, namely the True and False labelled ones.\n",
    "\n",
    "Whenever a frame does not satisfy the similarity criterias, we do not compute similarity for the next couples frames because these are likely to give the same results considering there is very little change between two successive images in a video. This allows us to slightly reduce time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO hyperparam search on these ?\n",
    "LAST_SEEN_NB_FRAMES = 40\n",
    "\n",
    "\n",
    "def generate_dataset(read_path, write_path, encoder, min_similarity, max_similarity, nb_samples=300 , train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Creates samples for training and validation sets, using an image similarity criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    images_path = read_path + '.png'\n",
    "    images_path = np.random.permutation(glob.glob(images_path))\n",
    "    images_path = [img for img in images_path if not any([(id_ in img) for id_ in test_ids])] # ignore images used for testing\n",
    "    \n",
    "    videos_path = read_path + '.mp4'\n",
    "    videos_path = set(glob.glob(videos_path)) # transformed to set for faster queries\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    # Writing training and validation images to respective folders\n",
    "    for image_path in images_path:\n",
    "        # Finding corresponding video\n",
    "        success, video_path = find_video(image_path, videos_path)\n",
    "        if success == True:\n",
    "            if sample_count >= nb_samples:\n",
    "                break\n",
    "            else:\n",
    "                count = extract_similar_image(image_path, video_path, f\"{write_path}train/\", encoder, min_similarity, max_similarity)\n",
    "                sample_count += count\n",
    "    \n",
    "    # Moving some images to to validation folder\n",
    "    nb_samples_val = int(sample_count - (sample_count * train_ratio)) # mandatory due to floating point precision\n",
    "    move_to_val(f\"{write_path}train/\", f\"{write_path}val/\", nb_samples_val)\n",
    "    \n",
    "    if sample_count < nb_samples:\n",
    "        print(f\"Could not obtain enough samples: \\nObtained : {sample_count} \\nDemanded: {nb_samples}\")\n",
    "             \n",
    "            \n",
    "def move_to_val(img_path, val_path, nb_samples_val):\n",
    "    \"\"\"\n",
    "    Move nb_samples_val images from the img_path folder to the val_path folder\n",
    "    \"\"\"\n",
    "    img_to_move_false = glob.glob(img_path+'false/*')[:nb_samples_val]\n",
    "    img_to_move_true = glob.glob(img_path+'true/*')[:nb_samples_val]\n",
    "    val_path_false = val_path + 'false'\n",
    "    val_path_true = val_path + 'true'\n",
    "    \n",
    "    for file_false, file_true in zip(img_to_move_false, img_to_move_true):\n",
    "        !mv $file_false $val_path_false\n",
    "        !mv $file_true $val_path_true\n",
    "\n",
    "        \n",
    "def extract_similar_image(image_path, video_path, write_path, encoder, min_similarity, max_similarity, mask=mask):\n",
    "    \"\"\"\n",
    "    For a given image, extract a similar sample from the corresponding video (if it exists) and save it.\n",
    "    \"\"\"\n",
    "    # Extracting and encoding the reference expert image\n",
    "    exp_image = cv2.imread(image_path)\n",
    "    exp_image_code = encoder(exp_image).reshape(-1)\n",
    "    \n",
    "    # Video exploitation\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    last_seen = LAST_SEEN_NB_FRAMES # variable to avoid considering two sample images that are almost the same \n",
    "    extraction_success = Fadlse\n",
    "    \n",
    "    while(video.isOpened()):\n",
    "        success, sample_image = video.read()\n",
    "        if (success == True):\n",
    "            if (last_seen >= LAST_SEEN_NB_FRAMES) :\n",
    "                # If we have access to an image, we encode it and verify the similarity to the expert image\n",
    "                sample_image_code = encoder(sample_image).reshape(-1)\n",
    "                similarity = cosine_similarity(exp_image_code, sample_image_code)\n",
    "                # If the similarity criterion is satisfied, the image is selected\n",
    "                if (similarity > min_similarity and similarity < max_similarity):\n",
    "                    \n",
    "                    # Saving sampled image\n",
    "                    _, image_id = os.path.split(image_path)\n",
    "                    cv2.imwrite(f\"{write_path}false/{image_id}\", resize_crop(sample_image, mask))\n",
    "                    \n",
    "                    # Saving corresponding expert image\n",
    "                    cv2.imwrite(f\"{write_path}true/{image_id}\", resize_crop(exp_image, mask))\n",
    "                    \n",
    "                    extraction_success = True\n",
    "                    video.release()\n",
    "                    break # cv2 does not release video fast enough\n",
    "                else:\n",
    "                    last_seen = 0\n",
    "        else:\n",
    "            #need to get out the while loop if we can't read a file\n",
    "            video.release()\n",
    "        last_seen += 1\n",
    "    \n",
    "    return extraction_success\n",
    "\n",
    "        \n",
    "def find_video(image_path, videos_path):\n",
    "    \"\"\"\n",
    "    For a given unique image path, find if there is corresponding video in the given list of videos paths\n",
    "    \n",
    "    image_path: str, path of single image\n",
    "    videos_path: set containing paths of all videos\n",
    "    return: bool, str (operation success, video path)\n",
    "    \"\"\"\n",
    "    video_path = image_path[:-4] + '.mp4'\n",
    "    if(video_path in videos_path):\n",
    "        return True, video_path\n",
    "    else:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_bounds = [[0.75, 0.85], [0.8, 0.9], [0.85, 0.95], [0.9, 0.97]]\n",
    "cnn = CnnEncoder()\n",
    "cnn = cnn.eval()\n",
    "encoders = [(create_brightness_matrix, 'brightMat'), (cnn, 'cnn')]\n",
    "for bounds in similarity_bounds:\n",
    "    for encoder in encoders:\n",
    "        dataset_path = f'data/butter_proc_images_{encoder[1]}_{bounds[0]}_{bounds[1]}/'\n",
    "        make_directory(dataset_path)\n",
    "        generate_dataset(read_path='data/Ultrason_butterflynetwork/*',\n",
    "                         write_path=dataset_path,\n",
    "                         encoder=encoder[0],\n",
    "                         min_similarity=bounds[0],\n",
    "                         max_similarity=bounds[1],\n",
    "                         nb_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training\n",
    "\n",
    "TODO pytorch tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_data(data_path):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_path, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'val']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                                 shuffle=True, num_workers=4)\n",
    "                  for x in ['train', 'val']}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    \n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "    \n",
    "def get_model_and_parameters(model_type):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    # Loading pretrained model\n",
    "    if(model_type is \"resnet18\"):\n",
    "        model = models.resnet18(pretrained=True)\n",
    "    elif(model_type is \"googlenet\"):\n",
    "        model = models.googlenet(pretrained=True)\n",
    "    else:\n",
    "        raise Error(f\"Model {model_type} is not supported\")\n",
    "        \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    \n",
    "    return model, criterion, optimizer_ft, exp_lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    losses_tr = []\n",
    "    losses_val = []\n",
    "    acc_tr = []\n",
    "    acc_val = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Storing loss and accuracy values\n",
    "            if phase == 'train':\n",
    "                losses_tr.append(epoch_loss)\n",
    "                acc_tr.append(epoch_acc)\n",
    "            else:\n",
    "                losses_val.append(epoch_loss)\n",
    "                acc_val.append(epoch_acc)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}\\n'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, losses_tr, losses_val, acc_tr, acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models\n",
    "! mkdir models/similarity\n",
    "! mkdir figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"resnet18\"\n",
    "\n",
    "for bounds in similarity_bounds:\n",
    "    for encoder in encoders:\n",
    "        dataset_path = f'data/butter_proc_images_{encoder[1]}_{bounds[0]}_{bounds[1]}/'\n",
    "        dataloaders, dataset_sizes, class_names = get_data(dataset_path)\n",
    "        model, criterion, optimizer_ft, exp_lr_scheduler = get_model_and_parameters(model_type)\n",
    "        \n",
    "        print(f'\\n ########## Encoder ({encoder[1]}) with similarity bounds ({bounds[0]}) and ({bounds[1]}) ##########\\n')\n",
    "        \n",
    "        # Carbon footprint measurement\n",
    "        cumulator = base.Cumulator()\n",
    "        cumulator.on()\n",
    "        \n",
    "        # Training model\n",
    "        model_trained, losses_tr, losses_val, acc_tr, acc_val = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                                    num_epochs=25)\n",
    "        cumulator.off()\n",
    "        print(f\"Carbon footprint : {cumulator.total_carbon_footprint():.2f} gCO2eq\")\n",
    "        \n",
    "        # Saving model\n",
    "        torch.save(model.state_dict(), f\"models/similarity/{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}\")\n",
    "        \n",
    "        # Plotting\n",
    "        plot(losses_tr, losses_val, 'Training loss', 'Validation loss',\n",
    "             f'Losses {bounds[0]}_{bounds[1]}', \n",
    "             f'figures/losses_{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}.png', 'loss')\n",
    "        plot(acc_tr, acc_val, 'Training accuracy', 'Validation accuracy',\n",
    "             f'Accuracy {bounds[0]}_{bounds[1]}',\n",
    "             f'figures/accuracy_{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}.png', 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model testing\n",
    "\n",
    "With our data correctly labeled and organised, we can now proceed to training our model. Training is done in the ```classifier.ipynb``` notebook. In this notebook we directly load our trained models and test them.\n",
    "\n",
    "### 4.1 Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(load_path, device, model_type):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    # Loading model architecture\n",
    "    if(model_type is \"resnet18\"):\n",
    "        model = models.resnet18(pretrained=True)\n",
    "    elif(model_type is \"googlenet\"):\n",
    "        model = models.googlenet(pretrained=True)\n",
    "    else:\n",
    "        raise Error(f\"Model {model_type} is not supported\")\n",
    "        \n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "    # Loading parameters to the model\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Testing\n",
    "\n",
    "In order to test our model's extraction power we once again use the notion of similarity. For our selected test images, we iterate through the corresponding video running each frame into our model. The one frame that achieves highest confidence as a \"True\" image is to be selected. We then compute the similarity between our selected image and the expert one. This measure is our testing metric. The higher, the closer our model as been able to emulate the expert's decision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_model(img):\n",
    "    \"\"\"\n",
    "    Read image and apply required transformations for model usage\n",
    "    \"\"\"\n",
    "    # Read, resize and crop\n",
    "    img = resize_crop(img, mask)\n",
    "    \n",
    "    # Transform to PIL format\n",
    "    img = Image.fromarray(img)\n",
    "    \n",
    "    # Images transformations to apply before entering model\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    return transform(img)\n",
    "\n",
    "\n",
    "def test_model(model, test_folder_path, encoder):\n",
    "    \"\"\"\n",
    "    Test the given model on images and videos contained in the given folder, based on a similarity metric\n",
    "    Return the average similarity between expert images and model selectd images from the corresponding videos\n",
    "    \"\"\"\n",
    "    images_path = test_folder_path + 'test/*.png'\n",
    "    images_path = glob.glob(images_path)\n",
    "    \n",
    "    videos_path = test_folder_path + 'test/*.mp4'\n",
    "    videos_path = set(glob.glob(videos_path)) # transformed to set for faster queries\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    # Use model on image every 'skip' frames\n",
    "    skip = 10\n",
    "    # TODO debug stuff\n",
    "    #images_path = images_path[:3]\n",
    "    #print(images_path)\n",
    "    \n",
    "    for image_path in images_path:\n",
    "        # Finding corresponding video\n",
    "        success, video_path = find_video(image_path, videos_path)\n",
    "        if(success == True):\n",
    "            # Scanning the video\n",
    "            video = cv2.VideoCapture(video_path)\n",
    "            frame = 0\n",
    "            best_confidence = 0\n",
    "            selected_image = None\n",
    "            while(video.isOpened()):\n",
    "                success, sample_image = video.read()\n",
    "                if(success == True):\n",
    "                    frame += 1\n",
    "                    if(frame % skip == 0):\n",
    "                        # Sampled image goes through the model\n",
    "                        model_image = prepare_for_model(sample_image)\n",
    "                        model_image = torch.unsqueeze(model_image, 0) # adding dummy batch dimension\n",
    "                        model_image = model_image.to(device)\n",
    "                        probabilities = nn.functional.softmax(model(model_image), dim = 1)\n",
    "                        confidence = probabilities[0,1]\n",
    "\n",
    "                        if(confidence >= best_confidence):\n",
    "                            best_confidence = confidence\n",
    "                            selected_image = sample_image.copy()\n",
    "                else:\n",
    "                    video.release()\n",
    "            \n",
    "            # Computing similarity\n",
    "            if selected_image is not None:\n",
    "                exp_image = cv2.imread(image_path)\n",
    "                exp_image_code = encoder(exp_image).reshape(-1)\n",
    "                selected_image_code = encoder(selected_image).reshape(-1)\n",
    "                similarity = cosine_similarity(exp_image_code, selected_image_code)\n",
    "                similarities.append(similarity)\n",
    "    \n",
    "    # Averaging similarity TODO think of another method ?\n",
    "    return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing average similarity over the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"resnet18\"\n",
    "test_folder_path = \"data/test/\"\n",
    "\n",
    "for bounds in similarity_bounds:\n",
    "    for test_encoder in encoders:\n",
    "        for encoder in encoders:\n",
    "            # Loading model\n",
    "            model_path = f'models/similarity/{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}'\n",
    "            model = load_model(model_path, device, model_type)\n",
    "            print(f'\\n ########## Model trained with encoder ({encoder[1]}) and similarity bounds ({bounds[0]}) - ({bounds[1]}), test similarity through {test_encoder[1]} ##########\\n')\n",
    "\n",
    "            # Testing\n",
    "            mean_similarity = test_model(model, test_folder_path, test_encoder[0])\n",
    "            print(f'Mean similarity on test set: {mean_similarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confident_images(model, test_folder_path, write_path):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"   \n",
    "    videos_path = test_folder_path + 'test/*.mp4'\n",
    "    videos_path = glob.glob(videos_path)\n",
    "    \n",
    "    good_counter = 0\n",
    "    avg_counter = 0\n",
    "    bad_counter = 0\n",
    "    \n",
    "    # Use model on image every 'skip' frames\n",
    "    skip = 1\n",
    "    # TODO debug stuff\n",
    "    #images_path = images_path[:3]\n",
    "    #print(images_path)\n",
    "    \n",
    "    for video_path in videos_path:\n",
    "        # Scanning the video\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        frame = 0\n",
    "        best_confidence = 0\n",
    "        min_best_confidence = 0.7\n",
    "        selected_image = None\n",
    "        average_image = None\n",
    "        bad_image = None\n",
    "        while(video.isOpened()):\n",
    "            success, sample_image = video.read()\n",
    "            if(success == True):\n",
    "                frame += 1\n",
    "                if(frame % skip == 0):\n",
    "                    # Sampled image goes through the model\n",
    "                    model_image = prepare_for_model(sample_image)\n",
    "                    model_image = torch.unsqueeze(model_image, 0) # adding dummy batch dimension\n",
    "                    model_image = model_image.to(device)\n",
    "                    probabilities = nn.functional.softmax(model(model_image), dim = 1)\n",
    "                    confidence = probabilities[0,1]\n",
    "                    if((confidence >= min_best_confidence) and (confidence >= best_confidence)):\n",
    "                        best_confidence = confidence\n",
    "                        selected_image = sample_image.copy()\n",
    "                    \n",
    "                    if((average_image is None) and ((confidence < 0.7) and (confidence > 0.2))):\n",
    "                        average_image = sample_image.copy()\n",
    "                        \n",
    "                    if((bad_image is None) and (confidence < 0.2)):\n",
    "                        bad_image = sample_image.copy()                         \n",
    "            else:\n",
    "                video.release()\n",
    "\n",
    "        # Writing images\n",
    "        image_id = os.path.split(video_path)[-1][:-4] + '.png'\n",
    "        if selected_image is not None:\n",
    "            good_counter +=1\n",
    "            cv2.imwrite(f\"{write_path}best/{image_id}\", resize_crop(selected_image, mask))\n",
    "        \n",
    "        if average_image is not None:\n",
    "            avg_counter +=1\n",
    "            cv2.imwrite(f\"{write_path}average/{image_id}\", resize_crop(average_image, mask))\n",
    "        \n",
    "        if bad_image is not None:\n",
    "            bad_counter +=1\n",
    "            cv2.imwrite(f\"{write_path}bad/{image_id}\", resize_crop(bad_image, mask))\n",
    "            \n",
    "    print(\"Good\", good_counter)\n",
    "    print(\"Average\", avg_counter)\n",
    "    print(\"Bad\", bad_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/selected_images\n",
    "! mkdir data/selected_images/best\n",
    "! mkdir data/selected_images/average\n",
    "! mkdir data/selected_images/bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'models/similarity/resnet18_natural_cnn_0.8_0.9'\n",
    "model = load_model(model_path, device, model_type)\n",
    "generate_confident_images(model, test_folder_path, 'data/selected_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bounds in similarity_bounds:\n",
    "    for encoder in encoders:\n",
    "        # Loading model\n",
    "        model_path = f'models/similarity/{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}'\n",
    "        model = load_model(model_path, device, model_type)\n",
    "        print(f'\\n ########## Model trained with encoder ({encoder[1]}) and similarity bounds ({bounds[0]}) - ({bounds[1]}) ##########\\n')\n",
    "        generate_confident_images(model, test_folder_path, 'data/selected_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_confidence_test_set_images(model, test_folder_path):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"   \n",
    "    images_path = test_folder_path + 'test/*.png'\n",
    "    images_path = glob.glob(images_path)\n",
    "    \n",
    "    \n",
    "    # Use model on image every 'skip' frames\n",
    "    skip = 10\n",
    "    # TODO debug stuff\n",
    "    #images_path = images_path[:3]\n",
    "    #print(images_path)\n",
    "    \n",
    "    good_counter = 0\n",
    "    avg_counter = 0\n",
    "    bad_counter = 0\n",
    "    \n",
    "    for image_path in images_path:\n",
    "        sample_image = cv2.imread(image_path)\n",
    "        # Sampled image goes through the model\n",
    "        model_image = prepare_for_model(sample_image)\n",
    "        model_image = torch.unsqueeze(model_image, 0) # adding dummy batch dimension\n",
    "        model_image = model_image.to(device)\n",
    "        probabilities = nn.functional.softmax(model(model_image), dim = 1)\n",
    "        confidence = probabilities[0,1]\n",
    "        \n",
    "        if confidence > 0.7:\n",
    "            good_counter += 1\n",
    "        elif confidence < 0.2:\n",
    "            bad_counter +=1\n",
    "        else:\n",
    "            avg_counter += 1\n",
    "            \n",
    "    print(\"Good\", good_counter)\n",
    "    print(\"Average\", avg_counter)\n",
    "    print(\"Bad\", bad_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'models/similarity/resnet18_natural_brightMat_0.85_0.95'\n",
    "model = load_model(model_path, device, model_type)\n",
    "give_confidence_test_set_images(model, test_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'models/similarity/resnet18_natural_cnn_0.75_0.85'\n",
    "model = load_model(model_path, device, model_type)\n",
    "give_confidence_test_set_images(model, test_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'models/similarity/resnet18_natural_cnn_0.8_0.9'\n",
    "model = load_model(model_path, device, model_type)\n",
    "give_confidence_test_set_images(model, test_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CnnEncoder()\n",
    "cnn = cnn.eval()\n",
    "img1 = cnn(cv2.imread('data/Ultrason_butterflynetwork/1.21_QLD.png')).reshape(-1)\n",
    "\n",
    "img2 = (np.random.rand(len(img1)) * 256).reshape(-1)\n",
    "cosine_similarity(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test = cv2.imread('data/test/test/1.16_QAID.png')\n",
    "image_test = resize_crop(image_test, mask)\n",
    "cv2.imwrite('image_test.png', image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('data/test/test/1.16_QAID.png')\n",
    "img1.resize(1080,792,3)\n",
    "print(len(img1.flatten()))\n",
    "cosine_similarity(img1[500,200:210,0].reshape(-1), image_test[500,200:210,0].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('data/test/test/1.16_QAID.png')\n",
    "img1[500,200:210,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test[500,200:210,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('data/test/test/1.16_QAID.png')\n",
    "print(img1.shape)\n",
    "print(img1[400,200:210,0])\n",
    "img1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1.resize(1080,792,3)\n",
    "print(img1.shape)\n",
    "print(img1[400,200:210,0])\n",
    "img1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lus.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

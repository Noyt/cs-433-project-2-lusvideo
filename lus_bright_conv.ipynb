{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YapYOP_srblT"
   },
   "source": [
    "# LUS-CS433\n",
    "\n",
    "TODO general project description + do a readme\n",
    "\n",
    "For this task, we decide to explore a supervised learning approach, using CNNs.\n",
    "Our initial idea is the following: to train a binary classifier ideally able to emulate a clinician's judgement, namely telling us wether an image taken from an LUS video of a patient site should be selected or not.\n",
    "\n",
    "TODO explain why classifier is multi site, wether we should do multiple classifiers (one per site)\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TZoIvD9nJuD1"
   },
   "outputs": [],
   "source": [
    "from encoders import CnnEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBJ5-wpVMirO"
   },
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "TODO some data exploration (number of images, videos, formats, etc)\n",
    "\n",
    "explain the concept of patient site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5jpXz_R_cP8"
   },
   "source": [
    "## 2. Creating Datasets\n",
    "\n",
    "In order to train our classifier, we require an image dataset with two different kind of labels:\n",
    "\n",
    "* \"True\" images: selected by expert clinicians, these images are known to well represent a patient site and its useful features (such as pleural line, A-lines, B-lines ... etc).\n",
    "* \"False\" images: taken from an LUS video, these images were not selected by an expert clinicians thus they do not represent the perfect shot for a given patient site (image can be blurry, features could be missing or angle shot can be weird and thus image is difficult to interpret).\n",
    "\n",
    "Our initial data contains roughly a thousand expert selected images, which intuitively form our \"True\" labeled images. However, we do not have any image that would correspond to our \"False\" label. Therefore we have to generate them ourselves. We use the following methodology to accomplish our goal:\n",
    "\n",
    "For a given \"True\" image, we iterate through images from its corresponding video. For each image, we compute a similarity value to the expert image and select the image as a False-labelled one based on this criteria. We require that the expert image and the selected image are not too similar to each other (since it would make the classification too hard), yet they should not be too different either (in order to better simulate the clinician options when selecting the best image). The details of the similarity metric are explained in later sections. \n",
    "\n",
    "\n",
    "### 2.1 Creating file folders\n",
    "\n",
    "First of all, we need to create folders that will contain our data. These follow a specific architecture that enables us to use PyTorch Datasets and Dataloaders classes conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CAq_xxIcQAHU"
   },
   "outputs": [],
   "source": [
    "! mkdir data/butter_proc_images_similarity\n",
    "! mkdir data/butter_proc_images_similarity/train\n",
    "! mkdir data/butter_proc_images_similarity/val\n",
    "! mkdir data/butter_proc_images_similarity/test\n",
    "! mkdir data/butter_proc_images_similarity/test/test\n",
    "! mkdir data/butter_proc_images_similarity/train/true\n",
    "! mkdir data/butter_proc_images_similarity/train/false\n",
    "! mkdir data/butter_proc_images_similarity/val/true\n",
    "! mkdir data/butter_proc_images_similarity/val/false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Image preprocessing\n",
    "\n",
    "We start by creating a mask that we will use to crop images, getting rid of features on the images (such as the scale, or name of site) that could otherwise perturb the training.\n",
    "The mask has been crafted manually, as we did not have time to implement automatic edge detrection or similar mechanism to automate the process. We also define a couple functions for common preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main mask used to capture the relevant portion of LUS images. Crafted manually. Is [1,1,1] where image is relevant\n",
    "\n",
    "# Image dimensions\n",
    "nb_rows = 1080\n",
    "nb_cols = 792\n",
    "nb_channels = 3\n",
    "\n",
    "mask = np.zeros([nb_rows, nb_cols, nb_channels])\n",
    "\n",
    "# Filling mask\n",
    "for row in range(nb_rows):\n",
    "    for col in range(nb_cols):\n",
    "        # Delimitations of the cone like portion of a LUS image\n",
    "        if row > 25 and row < 1010 and col < 762 and (-4/5 * row + 293) < col and (4/5*row) + nb_cols-293 > col:\n",
    "            mask[row, col] = [1,1,1]\n",
    "\n",
    "mask = mask.astype('uint8')\n",
    "            \n",
    "            \n",
    "def resize_crop(image, mask, nb_cols=792, nb_rows=1080):\n",
    "    \"\"\"\n",
    "    Resize image and apply mask\n",
    "    \"\"\"\n",
    "    masked_img = cv2.resize(image, (nb_cols, nb_rows))*mask\n",
    "    return masked_img\n",
    "\n",
    "\n",
    "def read_crop(image_path, mask, nb_cols=792, nb_rows=1080):\n",
    "    \"\"\"\n",
    "    Read image, resize to given dimensions and apply mask.\n",
    "    \n",
    "    Returns: image name, image\n",
    "    \"\"\"\n",
    "    # Reading and masking image\n",
    "    cv2_img = cv2.imread(image_path)\n",
    "    masked_img = resize_crop(cv2_img, mask, nb_cols, nb_rows)\n",
    "\n",
    "    # Selecting image name from path name\n",
    "    img_name = os.path.split(image_path)[-1]\n",
    "    \n",
    "    return img_name, masked_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Obtaining Test images\n",
    "\n",
    "Before any dataset generation, we need to select and exclude a few images and video for our testing purposes. We selected five patient ids by hand. These patients present the advantage of having images and corresponding videos for most of the sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9HjtwX90Q985"
   },
   "outputs": [],
   "source": [
    "# patient ids used for the test set\n",
    "test_ids = ['_151_','_136_', '_117_', '_45_', '_16_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Obtaining Training and Validation images\n",
    "\n",
    "We now proceed to dataset generation. First let us define our image similarity mechanism. \n",
    "\n",
    "#### 2.2.1 Similarity between two images\n",
    "\n",
    "The similarity measure was inspired to us by ([Yang et al.](https://arxiv.org/abs/1706.04737), 2017). The idea is to encode a images to lesser dimensional format, reshape them as vector, and finally compute their cosine similarity. The encoding process is necessary to reduce complexity and also avoid potential harm caused by the curse of dimensionality (TODO Prove this). Encoding can be done in various ways. In our project, we selected two methods:\n",
    "* Computing the mean brightness for a couple subparts of the image, in a convolutional fashion.\n",
    "* Using a pretrained CNN model (resnet18) stripped from its last layers, as an encoder\n",
    "\n",
    "The function ```create_brightness_matrix``` implements the first method TODO\n",
    "Second method TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LzgnfG9xH3He"
   },
   "outputs": [],
   "source": [
    "def create_brightness_matrix(img, mask=mask, side_length=200):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    img = resize_crop(img, mask)\n",
    "    # half_side determines the number of pixels per step of the \"convolution\"\n",
    "    half_side = int(side_length / 2)\n",
    "    nb_rows = img.shape[0] // half_side\n",
    "    nb_cols = img.shape[1] // half_side\n",
    "    matrix = np.zeros((nb_rows, nb_cols))\n",
    "    \n",
    "    for i in range(nb_rows) :\n",
    "        for j in range(nb_cols) :\n",
    "            matrix[i, j] = np.mean(img[half_side * i : half_side * i + side_length, half_side * j : half_side * j + side_length]) / 3\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors of the same size\n",
    "    \"\"\"\n",
    "    return np.dot(vector1,vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))\n",
    "\n",
    "# TODO not used as of now\n",
    "def compute_similarity(img1, img2, encoder):\n",
    "    \"\"\"\n",
    "    Compute similarity between two images based on cosine similarity.\n",
    "    \n",
    "    img_path1, img_path2: np.arrays\n",
    "    encoder: function, retreives image and encodes as np.array\n",
    "    return: int, cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    vec1 = encoder(img1).reshape(-1)\n",
    "    vec2 = encoder(img2).reshape(-1)\n",
    "    return cosine_similiraty(vec1, vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring cosine similarity with a few images (TODO should we keep this?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-80681bae0861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Exploring some similarity values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mim1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/butter_proc_images_2/train/false/1_175_QLD_4.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mim2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/butter_proc_images_2/train/true/1_135_QPSD.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcompute_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_brightness_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# Exploring some similarity values\n",
    "im1 = cv2.imread('data/butter_proc_images_2/train/false/1_175_QLD_4.png')*mask\n",
    "im2 = cv2.imread('data/butter_proc_images_2/train/true/1_135_QPSD.png')*mask\n",
    "compute_similarity(im1, im2, create_brightness_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-11a752c7c235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mim1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/butter_proc_images_2/train/true/1_143_QPSD_1.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mim2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/butter_proc_images_2/train/true/1_135_QPSD.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcompute_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_brightness_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "im1 = cv2.imread('data/butter_proc_images_2/train/true/1_143_QPSD_1.png')*mask\n",
    "im2 = cv2.imread('data/butter_proc_images_2/train/true/1_135_QPSD.png')*mask\n",
    "compute_similarity(im1, im2, create_brightness_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Generating datasets\n",
    "\n",
    "Armed with an encoder and a similarity measure, we can now tackle the image selection problem. We will proceed as described in Section 2.\n",
    "\n",
    "The function ```generate_datasets``` iterates through images present in the given folder, namely the Ultrason butterflynetwork one. For each of these expert selected image, the function tries to find a corresponding (same patient, same site) video through the function ```find_video```. If such video is found, the ```extract_similar_image``` function then iterates through the video, computing the similarity measure between the video's frames and the expert image, in a sequential fashion. If a given frame satisfies the similarity criterias (not too different, not too similar), then both the expert image and the selected frames are saved in their respective folder, namely the True and False labelled ones.\n",
    "\n",
    "Whenever a frame does not satisfy the similarity criterias, we do not compute similarity for the next couples frames because these are likely to give the same results considering there is very little change between two successive images in a video. This allows us to slightly reduce time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO hyperparam search on these ?\n",
    "MIN_SIMILARITY = 0.85\n",
    "MAX_SIMILARITY = 0.95\n",
    "LAST_SEEN_NB_FRAMES = 40\n",
    "\n",
    "def generate_datasets(read_path, write_path, encoder=create_brightness_matrix, nb_samples=300 , train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Creates samples for training and validation sets, using an image similarity criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    images_path = read_path + '.png'\n",
    "    images_path = np.random.permutation(glob.glob(images_path))\n",
    "    images_path = [img for img in images_path if not any([(id_ in img) for id_ in test_ids])] # ignore images used for testing\n",
    "    \n",
    "    videos_path = read_path + '.mp4'\n",
    "    videos_path = set(glob.glob(videos_path)) # transformed to set for faster queries\n",
    "    \n",
    "    nb_samples_train = int(nb_samples * train_ratio)\n",
    "    sample_count = 0\n",
    "        \n",
    "    # Writing training and validation images to respective folders\n",
    "    for image_path in images_path:\n",
    "        # Finding corresponding video\n",
    "        success, video_path = find_video(image_path, videos_path)\n",
    "        if success == True:\n",
    "            if sample_count >= nb_samples:\n",
    "                break\n",
    "            elif sample_count < nb_samples_train:\n",
    "                count = extract_similar_image(image_path, video_path, f\"{write_path}train/\")\n",
    "                sample_count += count\n",
    "            else:\n",
    "                count = extract_similar_image(image_path, video_path, f\"{write_path}val/\")\n",
    "                sample_count += count\n",
    "                \n",
    "    if sample_count < nb_samples:\n",
    "        print(f\"Could not obtain enough samples: \\nObtained : {sample_count} \\nDemanded: {nb_samples}\")\n",
    "\n",
    "        \n",
    "def extract_similar_image(image_path, video_path, write_path, mask=mask, encoder=create_brightness_matrix):\n",
    "    \"\"\"\n",
    "    For a given image, extract a similar sample from the corresponding video (if it exists) and save it.\n",
    "    \"\"\"\n",
    "    # Extracting and encoding the reference expert image\n",
    "    exp_image = cv2.imread(image_path)\n",
    "    exp_image_code = encoder(exp_image).reshape(-1)\n",
    "    \n",
    "    # Video exploitation\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    last_seen = LAST_SEEN_NB_FRAMES # variable to avoid considering two sample images that are almost the same \n",
    "    extraction_success = False\n",
    "    \n",
    "    while(video.isOpened()):\n",
    "        success, sample_image = video.read()\n",
    "        if (success == True):\n",
    "            if (last_seen >= LAST_SEEN_NB_FRAMES) :\n",
    "                # If we have access to an image, we encode it and verify the similarity to the expert image\n",
    "                sample_image_code = encoder(sample_image).reshape(-1)\n",
    "                similarity = cosine_similarity(exp_image_code, sample_image_code)\n",
    "\n",
    "                # If the similarity criterion is satisfied, the image is selected\n",
    "                if (similarity > MIN_SIMILARITY and similarity < MAX_SIMILARITY):\n",
    "                    \n",
    "                    # Saving sampled image\n",
    "                    _, image_id = os.path.split(image_path)\n",
    "                    cv2.imwrite(f\"{write_path}false/{image_id}\", resize_crop(sample_image, mask))\n",
    "                    \n",
    "                    # Saving corresponding expert image\n",
    "                    cv2.imwrite(f\"{write_path}true/{image_id}\", resize_crop(exp_image, mask))\n",
    "                    \n",
    "                    extraction_success = True\n",
    "                    video.release()\n",
    "                else:\n",
    "                    last_seen = 0\n",
    "        else:\n",
    "            #need to get out the while loop if we can't read a file\n",
    "            video.release()\n",
    "        last_seen += 1\n",
    "    \n",
    "    return extraction_success\n",
    "\n",
    "        \n",
    "def find_video(image_path, videos_path):\n",
    "    \"\"\"\n",
    "    For a given unique image path, find if there is corresponding video in the given list of videos paths\n",
    "    \n",
    "    image_path: str, path of single image\n",
    "    videos_path: set containing paths of all videos\n",
    "    return: bool, str (operation success, video path)\n",
    "    \"\"\"\n",
    "    video_path = image_path[:-4] + '.mp4'\n",
    "    if(video_path in videos_path):\n",
    "        return True, video_path\n",
    "    else:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_datasets(read_path='data/Ultrason butterflynetwork/*', write_path='data/butter_proc_images_similarity/', nb_samples=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model testing\n",
    "\n",
    "With our data correctly labeled and organised, we can now proceed to training our model. Training is done in the ```classifier.ipynb``` notebook. In this notebook we directly load our trained models and test them.\n",
    "\n",
    "### 3.1 Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"models/similarity/bright_matrix/09_12_resnet18_natural_500_85_95\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading model architecture\n",
    "model_test = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_test.fc.in_features\n",
    "model_test.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Loading parameters to the model\n",
    "model_test.load_state_dict(torch.load(load_path))\n",
    "model_test = model_test.to(device)\n",
    "model_test.eval()\n",
    "\n",
    "# Images transformations to apply before entering model\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                #transforms.RandomHorizontalFlip\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456(),, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Testing\n",
    "\n",
    "In order to test our model's extraction power we once again use the notion of similarity. For our selected test images, we iterate through the corresponding video running each frame into our model. The one frame that achieves highest confidence as a \"True\" image is to be selected. We then compute the similarity between our selected image and the expert one. This measure is our testing metric. The higher, the closer our model as been able to emulate the expert's decision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_model(img, transform=transform):\n",
    "    \"\"\"\n",
    "    Read image and apply required transformations for model usage\n",
    "    \"\"\"\n",
    "    # Read, resize and crop\n",
    "    img = resize_crop(img, mask)\n",
    "    # Transform to PIL format\n",
    "    img = Image.fromarray(img)\n",
    "    # Model transformations\n",
    "    return transform(img)\n",
    "\n",
    "\n",
    "def test_model(model, test_folder_path, encoder):\n",
    "    \"\"\"\n",
    "    Test the given model on images and videos contained in the given folder, based on a similarity metric\n",
    "    Return the average similarity between expert images and model selectd images from the corresponding videos\n",
    "    \"\"\"\n",
    "    images_path = test_folder_path + 'test/*.png'\n",
    "    images_path = glob.glob(images_path)\n",
    "    \n",
    "    videos_path = test_folder_path + 'test/*.mp4'\n",
    "    videos_path = set(glob.glob(videos_path)) # transformed to set for faster queries\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    # Use model on image every 'skip' frames\n",
    "    skip = 10\n",
    "    # TODO debug stuff\n",
    "    #images_path = images_path[:3]\n",
    "    #print(images_path)\n",
    "    \n",
    "    for image_path in images_path:\n",
    "        # Finding corresponding video\n",
    "        success, video_path = find_video(image_path, videos_path)\n",
    "        if(success == True):\n",
    "            # Scanning the video\n",
    "            video = cv2.VideoCapture(video_path)\n",
    "            frame = 0\n",
    "            best_confidence = 0\n",
    "            selected_image = None\n",
    "            while(video.isOpened()):\n",
    "                success, sample_image = video.read()\n",
    "                if(success == True):\n",
    "                    frame += 1\n",
    "                    if(frame % skip == 0):\n",
    "                        # Sampled image goes through the model\n",
    "                        model_image = prepare_for_model(sample_image)\n",
    "                        model_image = torch.unsqueeze(model_image, 0) # adding dummy batch dimension\n",
    "                        model_image = model_image.to(device)\n",
    "                        probabilities = nn.functional.softmax(model(model_image), dim = 1)\n",
    "                        confidence = probabilities[0,1]\n",
    "\n",
    "                        if(confidence >= best_confidence):\n",
    "                            best_confidence = confidence\n",
    "                            selected_image = sample_image.copy()\n",
    "                else:\n",
    "                    video.release()\n",
    "            \n",
    "            # Computing similarity\n",
    "            if selected_image is not None:\n",
    "                exp_image = cv2.imread(image_path)\n",
    "                exp_image_code = encoder(exp_image).reshape(-1)\n",
    "                selected_image_code = encoder(selected_image).reshape(-1)\n",
    "                similarity = cosine_similarity(exp_image_code, selected_image_code)\n",
    "                similarities.append(similarity)\n",
    "    \n",
    "    # Averaging similarity TODO think of another method ?\n",
    "    return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing average similarity over the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9506286917158284"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_folder_path = \"data/butter_proc_images_similarity/test/\"\n",
    "test_model(model_test, test_folder_path, create_brightness_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9506286917158284"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO why is it different for two runs on the same set ?\n",
    "test_folder_path = \"data/butter_proc_images_similarity/test/\"\n",
    "test_model(model_test, test_folder_path, create_brightness_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9451249742779325"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_folder_path = \"data/butter_proc_images_similarity/test/\"\n",
    "test_model(model_test, test_folder_path, create_brightness_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CnnEncoder()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lus.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YapYOP_srblT"
   },
   "source": [
    "# LUS-CS433\n",
    "\n",
    "TODO general project description + do a readme\n",
    "\n",
    "For this task, we decide to explore a supervised learning approach, using CNNs.\n",
    "Our initial idea is the following: to train a binary classifier ideally able to emulate a clinician's judgement, namely telling us wether an image taken from an LUS video of a patient site should be selected or not.\n",
    "\n",
    "TODO explain why classifier is multi site, wether we should do multiple classifiers (one per site)\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "TZoIvD9nJuD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from encoders import CnnEncoder\n",
    "from cumulator import base\n",
    "from plots import plot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBJ5-wpVMirO"
   },
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "TODO some data exploration (number of images, videos, formats, etc)\n",
    "\n",
    "explain the concept of patient site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5jpXz_R_cP8"
   },
   "source": [
    "## 2. Creating Datasets\n",
    "\n",
    "In order to train our classifier, we require an image dataset with two different kind of labels:\n",
    "\n",
    "* \"True\" images: selected by expert clinicians, these images are known to well represent a patient site and its useful features (such as pleural line, A-lines, B-lines ... etc).\n",
    "* \"False\" images: taken from an LUS video, these images were not selected by an expert clinicians thus they do not represent the perfect shot for a given patient site (image can be blurry, features could be missing or angle shot can be weird and thus image is difficult to interpret).\n",
    "\n",
    "Our initial data contains roughly a thousand expert selected images, which intuitively form our \"True\" labeled images. However, we do not have any image that would correspond to our \"False\" label. Therefore we have to generate them ourselves. We use the following methodology to accomplish our goal:\n",
    "\n",
    "For a given \"True\" image, we iterate through images from its corresponding video. For each image, we compute a similarity value to the expert image and select the image as a False-labelled one based on this criteria. We require that the expert image and the selected image are not too similar to each other (since it would make the classification too hard), yet they should not be too different either (in order to better simulate the clinician options when selecting the best image). The details of the similarity metric are explained in later sections. \n",
    "\n",
    "\n",
    "### 2.1 Creating file folders\n",
    "\n",
    "First of all, we need to define a function that creates folders that will contain our data. These follow a specific architecture that enables us to use PyTorch Datasets and Dataloaders classes conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CAq_xxIcQAHU"
   },
   "outputs": [],
   "source": [
    "def make_directory(path):\n",
    "    \"\"\"\n",
    "    Create PyTorch compatible folder architecture\n",
    "    \"\"\"\n",
    "    ! mkdir $path\n",
    "    for kind in ['train/', 'val/']:\n",
    "        sub_path = path + kind\n",
    "        ! mkdir $sub_path\n",
    "        for label in ['true', 'false']:\n",
    "            full_path = sub_path + label\n",
    "            ! mkdir $full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Image preprocessing\n",
    "\n",
    "We start by creating a mask that we will use to crop images, getting rid of features on the images (such as the scale, or name of site) that could otherwise perturb the training.\n",
    "The mask has been crafted manually, as we did not have time to implement automatic edge detrection or similar mechanism to automate the process. We also define a couple functions for common preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main mask used to capture the relevant portion of LUS images. Crafted manually. Is [1,1,1] where image is relevant\n",
    "\n",
    "# Image dimensions\n",
    "nb_rows = 1080\n",
    "nb_cols = 792\n",
    "nb_channels = 3\n",
    "\n",
    "mask = np.zeros([nb_rows, nb_cols, nb_channels])\n",
    "\n",
    "# Filling mask\n",
    "for row in range(nb_rows):\n",
    "    for col in range(nb_cols):\n",
    "        # Delimitations of the cone like portion of a LUS image\n",
    "        if row > 25 and row < 1010 and col < 762 and (-4/5 * row + 293) < col and (4/5*row) + nb_cols-293 > col:\n",
    "            mask[row, col] = [1,1,1]\n",
    "\n",
    "mask = mask.astype('uint8')\n",
    "            \n",
    "            \n",
    "def resize_crop(image, mask, nb_cols=792, nb_rows=1080):\n",
    "    \"\"\"\n",
    "    Resize image and apply mask\n",
    "    \"\"\"\n",
    "    masked_img = cv2.resize(image, (nb_cols, nb_rows))*mask\n",
    "    return masked_img\n",
    "\n",
    "\n",
    "def read_crop(image_path, mask, nb_cols=792, nb_rows=1080):\n",
    "    \"\"\"\n",
    "    Read image, resize to given dimensions and apply mask.\n",
    "    \n",
    "    Returns: image name, image\n",
    "    \"\"\"\n",
    "    # Reading and masking image\n",
    "    cv2_img = cv2.imread(image_path)\n",
    "    masked_img = resize_crop(cv2_img, mask, nb_cols, nb_rows)\n",
    "\n",
    "    # Selecting image name from path name\n",
    "    img_name = os.path.split(image_path)[-1]\n",
    "    \n",
    "    return img_name, masked_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Obtaining Test images\n",
    "\n",
    "Before any dataset generation, we need to select and exclude a few images and video for our testing purposes. We selected five patient ids by hand. These patients present the advantage of having images and corresponding videos for most of the sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rewrite simply\n",
    "# Create test data folder\n",
    "! mkdir data/test\n",
    "! mkdir data/test/test\n",
    "\n",
    "# Temporary text file used for copying data purposes\n",
    "! > test_data.txt\n",
    "\n",
    "# patient ids used for the test set\n",
    "test_ids = ['.151_','.136_', '.117_', '.45_', '.16_']\n",
    "\n",
    "files_path = glob.glob(\"data/Ultrason_butterflynetwork/*\")\n",
    "test_files = [file for file in files_path if any([(id_ in file) for id_ in test_ids])]\n",
    "\n",
    "file = open(\"test_data.txt\", \"w\")\n",
    "for test_file in test_files:\n",
    "    file.write(test_file+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for file in $(cat test_data.txt); \n",
    "do\n",
    "cp $file data/test/test/\n",
    "done\n",
    "rm test_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Obtaining Training and Validation images\n",
    "\n",
    "We now proceed to dataset generation. First let us define our image similarity mechanism. \n",
    "\n",
    "#### 2.2.1 Similarity between two images\n",
    "\n",
    "The similarity measure was inspired to us by ([Yang et al.](https://arxiv.org/abs/1706.04737), 2017). The idea is to encode a images to lesser dimensional format, reshape them as vector, and finally compute their cosine similarity. The encoding process is necessary to reduce complexity and also avoid potential harm caused by the curse of dimensionality (TODO Prove this). Encoding can be done in various ways. In our project, we selected two methods:\n",
    "* Computing the mean brightness for a couple subparts of the image, in a convolutional fashion.\n",
    "* Using a pretrained CNN model (resnet18) stripped from its last layers, as an encoder\n",
    "\n",
    "The function ```create_brightness_matrix``` implements the first method TODO\n",
    "Second method TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LzgnfG9xH3He"
   },
   "outputs": [],
   "source": [
    "def create_brightness_matrix(img, mask=mask, side_length=200):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    img = resize_crop(img, mask)\n",
    "    # half_side determines the number of pixels per step of the \"convolution\"\n",
    "    half_side = int(side_length / 2)\n",
    "    nb_rows = img.shape[0] // half_side\n",
    "    nb_cols = img.shape[1] // half_side\n",
    "    matrix = np.zeros((nb_rows, nb_cols))\n",
    "    \n",
    "    for i in range(nb_rows) :\n",
    "        for j in range(nb_cols) :\n",
    "            matrix[i, j] = np.mean(img[half_side * i : half_side * i + side_length, half_side * j : half_side * j + side_length]) / 3\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors of the same size\n",
    "    \"\"\"\n",
    "    return np.dot(vector1,vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Generating datasets\n",
    "\n",
    "Armed with an encoder and a similarity measure, we can now tackle the image selection problem. We will proceed as described in Section 2.\n",
    "\n",
    "The function ```generate_datasets``` iterates through images present in the given folder, namely the Ultrason butterflynetwork one. For each of these expert selected image, the function tries to find a corresponding (same patient, same site) video through the function ```find_video```. If such video is found, the ```extract_similar_image``` function then iterates through the video, computing the similarity measure between the video's frames and the expert image, in a sequential fashion. If a given frame satisfies the similarity criterias (not too different, not too similar), then both the expert image and the selected frames are saved in their respective folder, namely the True and False labelled ones.\n",
    "\n",
    "Whenever a frame does not satisfy the similarity criterias, we do not compute similarity for the next couples frames because these are likely to give the same results considering there is very little change between two successive images in a video. This allows us to slightly reduce time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO hyperparam search on these ?\n",
    "LAST_SEEN_NB_FRAMES = 40\n",
    "\n",
    "\n",
    "def generate_dataset(read_path, write_path, encoder, min_similarity, max_similarity, nb_samples=300 , train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Creates samples for training and validation sets, using an image similarity criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    images_path = read_path + '.png'\n",
    "    images_path = np.random.permutation(glob.glob(images_path))\n",
    "    images_path = [img for img in images_path if not any([(id_ in img) for id_ in test_ids])] # ignore images used for testing\n",
    "    \n",
    "    videos_path = read_path + '.mp4'\n",
    "    videos_path = set(glob.glob(videos_path)) # transformed to set for faster queries\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    # Writing training and validation images to respective folders\n",
    "    for image_path in images_path:\n",
    "        # Finding corresponding video\n",
    "        success, video_path = find_video(image_path, videos_path)\n",
    "        if success == True:\n",
    "            if sample_count >= nb_samples:\n",
    "                break\n",
    "            else:\n",
    "                count = extract_similar_image(image_path, video_path, f\"{write_path}train/\", encoder, min_similarity, max_similarity)\n",
    "                sample_count += count\n",
    "    \n",
    "    # Moving some images to to validation folder\n",
    "    nb_samples_val = int(sample_count - (sample_count * train_ratio)) # mandatory due to floating point precision\n",
    "    move_to_val(f\"{write_path}train/\", f\"{write_path}val/\", nb_samples_val)\n",
    "    \n",
    "    if sample_count < nb_samples:\n",
    "        print(f\"Could not obtain enough samples: \\nObtained : {sample_count} \\nDemanded: {nb_samples}\")\n",
    "             \n",
    "            \n",
    "def move_to_val(img_path, val_path, nb_samples_val):\n",
    "    \"\"\"\n",
    "    Move nb_samples_val images from the img_path folder to the val_path folder\n",
    "    \"\"\"\n",
    "    img_to_move_false = glob.glob(img_path+'false/*')[:nb_samples_val]\n",
    "    img_to_move_true = glob.glob(img_path+'true/*')[:nb_samples_val]\n",
    "    val_path_false = val_path + 'false'\n",
    "    val_path_true = val_path + 'true'\n",
    "    \n",
    "    for file_false, file_true in zip(img_to_move_false, img_to_move_true):\n",
    "        !mv $file_false $val_path_false\n",
    "        !mv $file_true $val_path_true\n",
    "\n",
    "        \n",
    "def extract_similar_image(image_path, video_path, write_path, encoder, min_similarity, max_similarity, mask=mask):\n",
    "    \"\"\"\n",
    "    For a given image, extract a similar sample from the corresponding video (if it exists) and save it.\n",
    "    \"\"\"\n",
    "    # Extracting and encoding the reference expert image\n",
    "    exp_image = cv2.imread(image_path)\n",
    "    exp_image_code = encoder(exp_image).reshape(-1)\n",
    "    \n",
    "    # Video exploitation\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    last_seen = LAST_SEEN_NB_FRAMES # variable to avoid considering two sample images that are almost the same \n",
    "    extraction_success = False\n",
    "    \n",
    "    while(video.isOpened()):\n",
    "        success, sample_image = video.read()\n",
    "        if (success == True):\n",
    "            if (last_seen >= LAST_SEEN_NB_FRAMES) :\n",
    "                # If we have access to an image, we encode it and verify the similarity to the expert image\n",
    "                sample_image_code = encoder(sample_image).reshape(-1)\n",
    "                similarity = cosine_similarity(exp_image_code, sample_image_code)\n",
    "                # If the similarity criterion is satisfied, the image is selected\n",
    "                if (similarity > min_similarity and similarity < max_similarity):\n",
    "                    \n",
    "                    # Saving sampled image\n",
    "                    _, image_id = os.path.split(image_path)\n",
    "                    cv2.imwrite(f\"{write_path}false/{image_id}\", resize_crop(sample_image, mask))\n",
    "                    \n",
    "                    # Saving corresponding expert image\n",
    "                    cv2.imwrite(f\"{write_path}true/{image_id}\", resize_crop(exp_image, mask))\n",
    "                    \n",
    "                    extraction_success = True\n",
    "                    video.release()\n",
    "                    break # cv2 does not release video fast enough\n",
    "                else:\n",
    "                    last_seen = 0\n",
    "        else:\n",
    "            #need to get out the while loop if we can't read a file\n",
    "            video.release()\n",
    "        last_seen += 1\n",
    "    \n",
    "    return extraction_success\n",
    "\n",
    "        \n",
    "def find_video(image_path, videos_path):\n",
    "    \"\"\"\n",
    "    For a given unique image path, find if there is corresponding video in the given list of videos paths\n",
    "    \n",
    "    image_path: str, path of single image\n",
    "    videos_path: set containing paths of all videos\n",
    "    return: bool, str (operation success, video path)\n",
    "    \"\"\"\n",
    "    video_path = image_path[:-4] + '.mp4'\n",
    "    if(video_path in videos_path):\n",
    "        return True, video_path\n",
    "    else:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not obtain enough samples: \n",
      "Obtained : 309 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 114 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 509 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 300 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 582 \n",
      "Demanded: 1000\n",
      "Could not obtain enough samples: \n",
      "Obtained : 449 \n",
      "Demanded: 1000\n"
     ]
    }
   ],
   "source": [
    "similarity_bounds = [[0.8, 0.9], [0.85, 0.95], [0.9, 0.97]]\n",
    "cnn = CnnEncoder()\n",
    "cnn = cnn.eval()\n",
    "encoders = [(create_brightness_matrix, 'brightMat'), (cnn, 'cnn')]\n",
    "for bounds in similarity_bounds:\n",
    "    for encoder in encoders:\n",
    "        dataset_path = f'data/butter_proc_images_{encoder[1]}_{bounds[0]}_{bounds[1]}/'\n",
    "        make_directory(dataset_path)\n",
    "        generate_dataset(read_path='data/Ultrason_butterflynetwork/*',\n",
    "                         write_path=dataset_path,\n",
    "                         encoder=encoder[0],\n",
    "                         min_similarity=bounds[0],\n",
    "                         max_similarity=bounds[1],\n",
    "                         nb_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_data(data_path):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_path, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'val']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                                 shuffle=True, num_workers=4)\n",
    "                  for x in ['train', 'val']}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    \n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "    \n",
    "def get_model_and_parameters(model_type):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    # Loading pretrained model\n",
    "    if(model_type is \"resnet18\"):\n",
    "        model = models.resnet18(pretrained=True)\n",
    "    elif(model_type is \"googlenet\"):\n",
    "        model = models.googlenet(pretrained=True)\n",
    "    else:\n",
    "        raise Error(f\"Model {model_type} is not supported\")\n",
    "        \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    \n",
    "    return model, criterion, optimizer_ft, exp_lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    losses_tr = []\n",
    "    losses_val = []\n",
    "    acc_tr = []\n",
    "    acc_val = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Storing loss and accuracy values\n",
    "            if phase == 'train':\n",
    "                losses_tr.append(epoch_loss)\n",
    "                acc_tr.append(epoch_acc)\n",
    "            else:\n",
    "                losses_val.append(epoch_loss)\n",
    "                acc_val.append(epoch_acc)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}\\n'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, losses_tr, losses_val, acc_tr, acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\n",
      "mkdir: cannot create directory ‘models/similarity’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir models\n",
    "! mkdir models/similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ########## Encoder (brightMat) with similarity bounds (0.8) and (0.9) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.6218 Acc: 0.6895\n",
      "val Loss: 3.2029 Acc: 0.5000\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.5706 Acc: 0.7641\n",
      "val Loss: 0.9472 Acc: 0.6230\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.4096 Acc: 0.8206\n",
      "val Loss: 3.4386 Acc: 0.5000\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.3284 Acc: 0.8810\n",
      "val Loss: 0.6196 Acc: 0.6885\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.4139 Acc: 0.8548\n",
      "val Loss: 1.2775 Acc: 0.5656\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.5794 Acc: 0.8044\n",
      "val Loss: 12.7958 Acc: 0.5000\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.2870 Acc: 0.9113\n",
      "val Loss: 3.1203 Acc: 0.5000\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.2130 Acc: 0.9214\n",
      "val Loss: 0.0423 Acc: 0.9918\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1984 Acc: 0.9194\n",
      "val Loss: 0.0313 Acc: 1.0000\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1884 Acc: 0.9315\n",
      "val Loss: 0.0232 Acc: 1.0000\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1428 Acc: 0.9415\n",
      "val Loss: 0.0988 Acc: 0.9672\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1420 Acc: 0.9496\n",
      "val Loss: 0.0186 Acc: 1.0000\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1284 Acc: 0.9516\n",
      "val Loss: 0.0419 Acc: 0.9918\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1179 Acc: 0.9556\n",
      "val Loss: 0.0140 Acc: 1.0000\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1191 Acc: 0.9617\n",
      "val Loss: 0.0238 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1502 Acc: 0.9315\n",
      "val Loss: 0.0230 Acc: 0.9918\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1138 Acc: 0.9597\n",
      "val Loss: 0.0120 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1565 Acc: 0.9395\n",
      "val Loss: 0.0133 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0995 Acc: 0.9516\n",
      "val Loss: 0.0143 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1380 Acc: 0.9315\n",
      "val Loss: 0.0139 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1084 Acc: 0.9536\n",
      "val Loss: 0.0109 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1266 Acc: 0.9435\n",
      "val Loss: 0.0142 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1611 Acc: 0.9315\n",
      "val Loss: 0.0141 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1382 Acc: 0.9395\n",
      "val Loss: 0.0177 Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1066 Acc: 0.9597\n",
      "val Loss: 0.0144 Acc: 1.0000\n",
      "\n",
      "Training complete in 2m 19s\n",
      "Best val Acc: 1.000000\n",
      "Carbon footprint : 4.30 gCO2eq\n",
      "\n",
      " ########## Encoder (cnn) with similarity bounds (0.8) and (0.9) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.6357 Acc: 0.6630\n",
      "val Loss: 0.4476 Acc: 0.7955\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.5165 Acc: 0.8043\n",
      "val Loss: 0.7221 Acc: 0.5909\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.3869 Acc: 0.8533\n",
      "val Loss: 0.7787 Acc: 0.6364\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2925 Acc: 0.9076\n",
      "val Loss: 0.7350 Acc: 0.5227\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.2568 Acc: 0.9022\n",
      "val Loss: 1.2543 Acc: 0.5000\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1507 Acc: 0.9511\n",
      "val Loss: 0.7459 Acc: 0.7727\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.3698 Acc: 0.8696\n",
      "val Loss: 0.6256 Acc: 0.7273\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.3084 Acc: 0.8641\n",
      "val Loss: 0.0487 Acc: 0.9773\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.2217 Acc: 0.9130\n",
      "val Loss: 0.0686 Acc: 0.9545\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.2193 Acc: 0.9239\n",
      "val Loss: 0.0331 Acc: 0.9773\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1932 Acc: 0.9239\n",
      "val Loss: 0.0210 Acc: 1.0000\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1527 Acc: 0.9239\n",
      "val Loss: 0.0158 Acc: 1.0000\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1733 Acc: 0.9130\n",
      "val Loss: 0.0279 Acc: 1.0000\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1253 Acc: 0.9511\n",
      "val Loss: 0.0890 Acc: 0.9773\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1983 Acc: 0.8913\n",
      "val Loss: 0.0381 Acc: 0.9773\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0461 Acc: 0.9783\n",
      "val Loss: 0.0212 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0667 Acc: 0.9620\n",
      "val Loss: 0.0178 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1543 Acc: 0.9293\n",
      "val Loss: 0.0165 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.2266 Acc: 0.8967\n",
      "val Loss: 0.0192 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1672 Acc: 0.9130\n",
      "val Loss: 0.0156 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1969 Acc: 0.9130\n",
      "val Loss: 0.0288 Acc: 0.9773\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1148 Acc: 0.9511\n",
      "val Loss: 0.0303 Acc: 0.9773\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1016 Acc: 0.9565\n",
      "val Loss: 0.0142 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0796 Acc: 0.9783\n",
      "val Loss: 0.0201 Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0818 Acc: 0.9565\n",
      "val Loss: 0.0339 Acc: 0.9773\n",
      "\n",
      "Training complete in 0m 56s\n",
      "Best val Acc: 1.000000\n",
      "Carbon footprint : 1.74 gCO2eq\n",
      "\n",
      " ########## Encoder (brightMat) with similarity bounds (0.85) and (0.95) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5340 Acc: 0.7696\n",
      "val Loss: 0.6303 Acc: 0.6733\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.4796 Acc: 0.8235\n",
      "val Loss: 1.9602 Acc: 0.5000\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.3839 Acc: 0.8468\n",
      "val Loss: 1.9755 Acc: 0.5000\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.3178 Acc: 0.8836\n",
      "val Loss: 1.1612 Acc: 0.5396\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.3269 Acc: 0.8848\n",
      "val Loss: 3.8734 Acc: 0.5000\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.2168 Acc: 0.9240\n",
      "val Loss: 0.7031 Acc: 0.5941\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.3014 Acc: 0.8971\n",
      "val Loss: 5.2840 Acc: 0.5000\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1722 Acc: 0.9314\n",
      "val Loss: 0.0127 Acc: 1.0000\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1266 Acc: 0.9522\n",
      "val Loss: 0.0617 Acc: 0.9901\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1646 Acc: 0.9289\n",
      "val Loss: 0.0194 Acc: 1.0000\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1407 Acc: 0.9412\n",
      "val Loss: 0.0645 Acc: 0.9703\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1259 Acc: 0.9449\n",
      "val Loss: 0.1848 Acc: 0.9158\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1462 Acc: 0.9375\n",
      "val Loss: 0.0103 Acc: 1.0000\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1523 Acc: 0.9289\n",
      "val Loss: 0.1526 Acc: 0.9554\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1119 Acc: 0.9498\n",
      "val Loss: 0.0149 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.9363\n",
      "val Loss: 0.0143 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1262 Acc: 0.9449\n",
      "val Loss: 0.0140 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1070 Acc: 0.9583\n",
      "val Loss: 0.0108 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1075 Acc: 0.9534\n",
      "val Loss: 0.0119 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1135 Acc: 0.9387\n",
      "val Loss: 0.0109 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1030 Acc: 0.9571\n",
      "val Loss: 0.0124 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1104 Acc: 0.9547\n",
      "val Loss: 0.0126 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1080 Acc: 0.9522\n",
      "val Loss: 0.0093 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1291 Acc: 0.9387\n",
      "val Loss: 0.0099 Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1135 Acc: 0.9522\n",
      "val Loss: 0.0097 Acc: 1.0000\n",
      "\n",
      "Training complete in 3m 45s\n",
      "Best val Acc: 1.000000\n",
      "Carbon footprint : 6.97 gCO2eq\n",
      "\n",
      " ########## Encoder (cnn) with similarity bounds (0.85) and (0.95) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5493 Acc: 0.7417\n",
      "val Loss: 1.9161 Acc: 0.5000\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.4546 Acc: 0.8292\n",
      "val Loss: 0.8603 Acc: 0.6167\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.3238 Acc: 0.8958\n",
      "val Loss: 0.4328 Acc: 0.8167\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.3694 Acc: 0.8688\n",
      "val Loss: 12.1479 Acc: 0.5000\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.3624 Acc: 0.8688\n",
      "val Loss: 0.5225 Acc: 0.7250\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.3288 Acc: 0.8812\n",
      "val Loss: 4.9561 Acc: 0.5000\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.3224 Acc: 0.8896\n",
      "val Loss: 3.3512 Acc: 0.5000\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1739 Acc: 0.9417\n",
      "val Loss: 0.0113 Acc: 1.0000\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1672 Acc: 0.9250\n",
      "val Loss: 0.0125 Acc: 1.0000\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1474 Acc: 0.9375\n",
      "val Loss: 0.0083 Acc: 1.0000\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1270 Acc: 0.9500\n",
      "val Loss: 0.0086 Acc: 1.0000\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1591 Acc: 0.9375\n",
      "val Loss: 0.0094 Acc: 1.0000\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1236 Acc: 0.9521\n",
      "val Loss: 0.0125 Acc: 1.0000\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.9396\n",
      "val Loss: 0.0066 Acc: 1.0000\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.0992 Acc: 0.9604\n",
      "val Loss: 0.0058 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1829 Acc: 0.9167\n",
      "val Loss: 0.0071 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.9646\n",
      "val Loss: 0.0070 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1206 Acc: 0.9521\n",
      "val Loss: 0.0072 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9229\n",
      "val Loss: 0.0049 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1618 Acc: 0.9229\n",
      "val Loss: 0.0062 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1323 Acc: 0.9458\n",
      "val Loss: 0.0102 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1479 Acc: 0.9354\n",
      "val Loss: 0.0067 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1205 Acc: 0.9396\n",
      "val Loss: 0.0071 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1155 Acc: 0.9479\n",
      "val Loss: 0.0087 Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1171 Acc: 0.9521\n",
      "val Loss: 0.0139 Acc: 1.0000\n",
      "\n",
      "Training complete in 2m 6s\n",
      "Best val Acc: 1.000000\n",
      "Carbon footprint : 3.92 gCO2eq\n",
      "\n",
      " ########## Encoder (brightMat) with similarity bounds (0.9) and (0.97) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5361 Acc: 0.7446\n",
      "val Loss: 0.6079 Acc: 0.6336\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.4025 Acc: 0.8509\n",
      "val Loss: 3.9604 Acc: 0.5000\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.3797 Acc: 0.8627\n",
      "val Loss: 3.6092 Acc: 0.5000\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.4123 Acc: 0.8584\n",
      "val Loss: 0.1201 Acc: 0.9698\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.4460 Acc: 0.8519\n",
      "val Loss: 4.5830 Acc: 0.5000\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.3406 Acc: 0.8745\n",
      "val Loss: 5.9674 Acc: 0.5000\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.2390 Acc: 0.9217\n",
      "val Loss: 8.7941 Acc: 0.5043\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1996 Acc: 0.9335\n",
      "val Loss: 0.0329 Acc: 1.0000\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1705 Acc: 0.9249\n",
      "val Loss: 4.4627 Acc: 0.5259\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1239 Acc: 0.9464\n",
      "val Loss: 0.0732 Acc: 0.9784\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1121 Acc: 0.9571\n",
      "val Loss: 0.0195 Acc: 1.0000\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1075 Acc: 0.9560\n",
      "val Loss: 0.2991 Acc: 0.8448\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.9603\n",
      "val Loss: 0.2271 Acc: 0.9052\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0921 Acc: 0.9603\n",
      "val Loss: 0.1466 Acc: 0.9353\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1072 Acc: 0.9517\n",
      "val Loss: 0.0091 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1028 Acc: 0.9496\n",
      "val Loss: 0.0173 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1058 Acc: 0.9506\n",
      "val Loss: 0.0149 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0983 Acc: 0.9582\n",
      "val Loss: 0.0213 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1232 Acc: 0.9388\n",
      "val Loss: 0.0077 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1276 Acc: 0.9464\n",
      "val Loss: 0.0073 Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1156 Acc: 0.9453\n",
      "val Loss: 0.0088 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0840 Acc: 0.9603\n",
      "val Loss: 0.0128 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0921 Acc: 0.9624\n",
      "val Loss: 0.0118 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0875 Acc: 0.9635\n",
      "val Loss: 0.0241 Acc: 0.9957\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0911 Acc: 0.9592\n",
      "val Loss: 0.0110 Acc: 1.0000\n",
      "\n",
      "Training complete in 3m 18s\n",
      "Best val Acc: 1.000000\n",
      "Carbon footprint : 6.14 gCO2eq\n",
      "\n",
      " ########## Encoder (cnn) with similarity bounds (0.9) and (0.97) ##########\n",
      "\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5587 Acc: 0.7639\n",
      "val Loss: 0.2632 Acc: 0.9101\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.3796 Acc: 0.8569\n",
      "val Loss: 0.6731 Acc: 0.6966\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.3849 Acc: 0.8597\n",
      "val Loss: 10.1909 Acc: 0.5000\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.6302 Acc: 0.8111\n",
      "val Loss: 12.9335 Acc: 0.5000\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.3612 Acc: 0.8764\n",
      "val Loss: 0.9881 Acc: 0.5730\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.4074 Acc: 0.8750\n",
      "val Loss: 2.2952 Acc: 0.5225\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.4137 Acc: 0.8833\n",
      "val Loss: 16.2156 Acc: 0.5000\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.2857 Acc: 0.8972\n",
      "val Loss: 0.3253 Acc: 0.8708\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1384 Acc: 0.9569\n",
      "val Loss: 0.6420 Acc: 0.7978\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1484 Acc: 0.9375\n",
      "val Loss: 0.0108 Acc: 1.0000\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1540 Acc: 0.9292\n",
      "val Loss: 0.1077 Acc: 0.9607\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.2109 Acc: 0.9097\n",
      "val Loss: 0.2066 Acc: 0.9213\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1411 Acc: 0.9417\n",
      "val Loss: 0.5500 Acc: 0.8090\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1202 Acc: 0.9486\n",
      "val Loss: 0.1617 Acc: 0.9157\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1019 Acc: 0.9653\n",
      "val Loss: 0.0101 Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1405 Acc: 0.9375\n",
      "val Loss: 0.0142 Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1521 Acc: 0.9250\n",
      "val Loss: 0.0112 Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1874 Acc: 0.9097\n",
      "val Loss: 0.0117 Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1102 Acc: 0.9667\n",
      "val Loss: 0.0135 Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1302 Acc: 0.9500\n",
      "val Loss: 0.0359 Acc: 0.9719\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1528 Acc: 0.9361\n",
      "val Loss: 0.0159 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1535 Acc: 0.9347\n",
      "val Loss: 0.0171 Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1383 Acc: 0.9389\n",
      "val Loss: 0.0246 Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1640 Acc: 0.9375\n",
      "val Loss: 0.0164 Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1361 Acc: 0.9500\n",
      "val Loss: 0.0185 Acc: 1.0000\n",
      "\n",
      "Training complete in 2m 45s\n",
      "Best val Acc: 1.000000\n",
      "Carbon footprint : 5.12 gCO2eq\n"
     ]
    }
   ],
   "source": [
    "# TODO reduntant with above cell\n",
    "similarity_bounds = [[0.8, 0.9], [0.85, 0.95], [0.9, 0.97]]\n",
    "cnn = CnnEncoder()\n",
    "cnn = cnn.eval()\n",
    "encoders = [(create_brightness_matrix, 'brightMat'), (cnn, 'cnn')]\n",
    "\n",
    "\n",
    "model_type = \"resnet18\"\n",
    "\n",
    "for bounds in similarity_bounds:\n",
    "    for encoder in encoders:\n",
    "        dataset_path = f'data/butter_proc_images_{encoder[1]}_{bounds[0]}_{bounds[1]}/'\n",
    "        dataloaders, dataset_sizes, class_names = get_data(dataset_path)\n",
    "        model, criterion, optimizer_ft, exp_lr_scheduler = get_model_and_parameters(model_type)\n",
    "        \n",
    "        print(f'\\n ########## Encoder ({encoder[1]}) with similarity bounds ({bounds[0]}) and ({bounds[1]}) ##########\\n')\n",
    "        \n",
    "        # Carbon footprint measurement\n",
    "        cumulator = base.Cumulator()\n",
    "        cumulator.on()\n",
    "        \n",
    "        # Training model\n",
    "        model_trained, losses_tr, losses_val, acc_tr, acc_val = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                                    num_epochs=25)\n",
    "        cumulator.off()\n",
    "        print(f\"Carbon footprint : {cumulator.total_carbon_footprint():.2f} gCO2eq\")\n",
    "        \n",
    "        # Saving model\n",
    "        torch.save(model.state_dict(), f\"models/similarity/{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}\")\n",
    "        \n",
    "        # Plotting\n",
    "        plot(losses_tr, losses_val, 'Training loss', 'Validation loss',\n",
    "             f'Losses {bounds[0]}_{bounds[1]}', \n",
    "             f'figures/losses_{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}.png', 'loss')\n",
    "        plot(acc_tr, acc_val, 'Training accuracy', 'Validation accuracy',\n",
    "             f'Accuracy {bounds[0]}_{bounds[1]}',\n",
    "             f'figures/accuracy_{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}.png', 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model testing\n",
    "\n",
    "With our data correctly labeled and organised, we can now proceed to training our model. Training is done in the ```classifier.ipynb``` notebook. In this notebook we directly load our trained models and test them.\n",
    "\n",
    "### 4.1 Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(load_path, device, model_type):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    # Loading model architecture\n",
    "    if(model_type is \"resnet18\"):\n",
    "        model = models.resnet18(pretrained=True)\n",
    "    elif(model_type is \"googlenet\"):\n",
    "        model = models.googlenet(pretrained=True)\n",
    "    else:\n",
    "        raise Error(f\"Model {model_type} is not supported\")\n",
    "        \n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "    # Loading parameters to the model\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Testing\n",
    "\n",
    "In order to test our model's extraction power we once again use the notion of similarity. For our selected test images, we iterate through the corresponding video running each frame into our model. The one frame that achieves highest confidence as a \"True\" image is to be selected. We then compute the similarity between our selected image and the expert one. This measure is our testing metric. The higher, the closer our model as been able to emulate the expert's decision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_model(img):\n",
    "    \"\"\"\n",
    "    Read image and apply required transformations for model usage\n",
    "    \"\"\"\n",
    "    # Read, resize and crop\n",
    "    img = resize_crop(img, mask)\n",
    "    \n",
    "    # Transform to PIL format\n",
    "    img = Image.fromarray(img)\n",
    "    \n",
    "    # Images transformations to apply before entering model\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    return transform(img)\n",
    "\n",
    "\n",
    "def test_model(model, test_folder_path, encoder):\n",
    "    \"\"\"\n",
    "    Test the given model on images and videos contained in the given folder, based on a similarity metric\n",
    "    Return the average similarity between expert images and model selectd images from the corresponding videos\n",
    "    \"\"\"\n",
    "    images_path = test_folder_path + 'test/*.png'\n",
    "    images_path = glob.glob(images_path)\n",
    "    \n",
    "    videos_path = test_folder_path + 'test/*.mp4'\n",
    "    videos_path = set(glob.glob(videos_path)) # transformed to set for faster queries\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    # Use model on image every 'skip' frames\n",
    "    skip = 10\n",
    "    # TODO debug stuff\n",
    "    #images_path = images_path[:3]\n",
    "    #print(images_path)\n",
    "    \n",
    "    for image_path in images_path:\n",
    "        # Finding corresponding video\n",
    "        success, video_path = find_video(image_path, videos_path)\n",
    "        if(success == True):\n",
    "            # Scanning the video\n",
    "            video = cv2.VideoCapture(video_path)\n",
    "            frame = 0\n",
    "            best_confidence = 0\n",
    "            selected_image = None\n",
    "            while(video.isOpened()):\n",
    "                success, sample_image = video.read()\n",
    "                if(success == True):\n",
    "                    frame += 1\n",
    "                    if(frame % skip == 0):\n",
    "                        # Sampled image goes through the model\n",
    "                        model_image = prepare_for_model(sample_image)\n",
    "                        model_image = torch.unsqueeze(model_image, 0) # adding dummy batch dimension\n",
    "                        model_image = model_image.to(device)\n",
    "                        probabilities = nn.functional.softmax(model(model_image), dim = 1)\n",
    "                        confidence = probabilities[0,1]\n",
    "\n",
    "                        if(confidence >= best_confidence):\n",
    "                            best_confidence = confidence\n",
    "                            selected_image = sample_image.copy()\n",
    "                else:\n",
    "                    video.release()\n",
    "            \n",
    "            # Computing similarity\n",
    "            if selected_image is not None:\n",
    "                exp_image = cv2.imread(image_path)\n",
    "                exp_image_code = encoder(exp_image).reshape(-1)\n",
    "                selected_image_code = encoder(selected_image).reshape(-1)\n",
    "                similarity = cosine_similarity(exp_image_code, selected_image_code)\n",
    "                similarities.append(similarity)\n",
    "    \n",
    "    # Averaging similarity TODO think of another method ?\n",
    "    return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing average similarity over the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.8) - (0.9), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9447\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.8) - (0.9), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9441\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.8) - (0.9), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9641\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.8) - (0.9), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9652\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.85) - (0.95), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9413\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.85) - (0.95), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9463\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.85) - (0.95), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9638\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.85) - (0.95), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9640\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.9) - (0.97), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9304\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.9) - (0.97), test similarity through brightMat ##########\n",
      "\n",
      "Mean similarity on test set: 0.9423\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.9) - (0.97), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9577\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.9) - (0.97), test similarity through cnn ##########\n",
      "\n",
      "Mean similarity on test set: 0.9620\n"
     ]
    }
   ],
   "source": [
    "model_type = \"resnet18\"\n",
    "test_folder_path = \"data/test/\"\n",
    "\n",
    "for bounds in similarity_bounds:\n",
    "    for test_encoder in encoders:\n",
    "        for encoder in encoders:\n",
    "            # Loading model\n",
    "            model_path = f'models/similarity/{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}'\n",
    "            model = load_model(model_path, device, model_type)\n",
    "            print(f'\\n ########## Model trained with encoder ({encoder[1]}) and similarity bounds ({bounds[0]}) - ({bounds[1]}), test similarity through {test_encoder[1]} ##########\\n')\n",
    "\n",
    "            # Testing\n",
    "            mean_similarity = test_model(model, test_folder_path, test_encoder[0])\n",
    "            print(f'Mean similarity on test set: {mean_similarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confident_images(model, test_folder_path, write_path):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"   \n",
    "    videos_path = test_folder_path + 'test/*.mp4'\n",
    "    videos_path = glob.glob(videos_path)\n",
    "    \n",
    "    good_counter = 0\n",
    "    avg_counter = 0\n",
    "    bad_counter = 0\n",
    "    \n",
    "    # Use model on image every 'skip' frames\n",
    "    skip = 3\n",
    "    # TODO debug stuff\n",
    "    #images_path = images_path[:3]\n",
    "    #print(images_path)\n",
    "    \n",
    "    for video_path in videos_path:\n",
    "        # Scanning the video\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        frame = 0\n",
    "        best_confidence = 0\n",
    "        min_best_confidence = 0.7\n",
    "        selected_image = None\n",
    "        average_image = None\n",
    "        bad_image = None\n",
    "        while(video.isOpened()):\n",
    "            success, sample_image = video.read()\n",
    "            if(success == True):\n",
    "                frame += 1\n",
    "                if(frame % skip == 0):\n",
    "                    # Sampled image goes through the model\n",
    "                    model_image = prepare_for_model(sample_image)\n",
    "                    model_image = torch.unsqueeze(model_image, 0) # adding dummy batch dimension\n",
    "                    model_image = model_image.to(device)\n",
    "                    probabilities = nn.functional.softmax(model(model_image), dim = 1)\n",
    "                    confidence = probabilities[0,1]\n",
    "                    if((confidence >= min_best_confidence) and (confidence >= best_confidence)):\n",
    "                        best_confidence = confidence\n",
    "                        selected_image = sample_image.copy()\n",
    "                    \n",
    "                    if((average_image is None) and ((confidence < 0.7) and (confidence > 0.2))):\n",
    "                        average_image = sample_image.copy()\n",
    "                        \n",
    "                    if((bad_image is None) and (confidence < 0.2)):\n",
    "                        bad_image = sample_image.copy()                         \n",
    "            else:\n",
    "                video.release()\n",
    "\n",
    "        # Writing images\n",
    "        image_id = os.path.split(video_path)[-1][:-4] + '.png'\n",
    "        if selected_image is not None:\n",
    "            good_counter +=1\n",
    "            #cv2.imwrite(f\"{write_path}best/{image_id}\", resize_crop(selected_image, mask))\n",
    "        \n",
    "        if average_image is not None:\n",
    "            avg_counter +=1\n",
    "            #cv2.imwrite(f\"{write_path}average/{image_id}\", resize_crop(average_image, mask))\n",
    "        \n",
    "        if bad_image is not None:\n",
    "            bad_counter +=1\n",
    "            #cv2.imwrite(f\"{write_path}bad/{image_id}\", resize_crop(bad_image, mask))\n",
    "            \n",
    "    print(\"Good\", good_counter)\n",
    "    print(\"Ugly\", avg_counter)\n",
    "    print(\"Bad\", bad_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.8) - (0.9) ##########\n",
      "\n",
      "Good 1\n",
      "Ugly 24\n",
      "Bad 74\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.8) - (0.9) ##########\n",
      "\n",
      "Good 13\n",
      "Ugly 32\n",
      "Bad 72\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.85) - (0.95) ##########\n",
      "\n",
      "Good 0\n",
      "Ugly 2\n",
      "Bad 74\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.85) - (0.95) ##########\n",
      "\n",
      "Good 2\n",
      "Ugly 23\n",
      "Bad 74\n",
      "\n",
      " ########## Model trained with encoder (brightMat) and similarity bounds (0.9) - (0.97) ##########\n",
      "\n",
      "Good 0\n",
      "Ugly 4\n",
      "Bad 74\n",
      "\n",
      " ########## Model trained with encoder (cnn) and similarity bounds (0.9) - (0.97) ##########\n",
      "\n",
      "Good 0\n",
      "Ugly 1\n",
      "Bad 74\n"
     ]
    }
   ],
   "source": [
    "for bounds in similarity_bounds:\n",
    "    for encoder in encoders:\n",
    "        # Loading model\n",
    "        model_path = f'models/similarity/{model_type}_natural_{encoder[1]}_{bounds[0]}_{bounds[1]}'\n",
    "        model = load_model(model_path, device, model_type)\n",
    "        print(f'\\n ########## Model trained with encoder ({encoder[1]}) and similarity bounds ({bounds[0]}) - ({bounds[1]}) ##########\\n')\n",
    "        generate_confident_images(model, test_folder_path, 'data/selected_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_confidence_test_set_images(model, test_folder_path):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"   \n",
    "    images_path = test_folder_path + 'test/*.png'\n",
    "    images_path = glob.glob(images_path)\n",
    "    \n",
    "    \n",
    "    # Use model on image every 'skip' frames\n",
    "    skip = 10\n",
    "    # TODO debug stuff\n",
    "    #images_path = images_path[:3]\n",
    "    #print(images_path)\n",
    "    \n",
    "    good_counter = 0\n",
    "    avg_counter = 0\n",
    "    bad_counter = 0\n",
    "    \n",
    "    for image_path in images_path:\n",
    "        sample_image = cv2.imread(image_path)\n",
    "        # Sampled image goes through the model\n",
    "        model_image = prepare_for_model(sample_image)\n",
    "        model_image = torch.unsqueeze(model_image, 0) # adding dummy batch dimension\n",
    "        model_image = model_image.to(device)\n",
    "        probabilities = nn.functional.softmax(model(model_image), dim = 1)\n",
    "        confidence = probabilities[0,1]\n",
    "        \n",
    "        if confidence > 0.7:\n",
    "            good_counter += 1\n",
    "        elif confidence < 0.2:\n",
    "            bad_counter +=1\n",
    "        else:\n",
    "            avg_counter += 1\n",
    "            \n",
    "    print(\"Good\", good_counter)\n",
    "    print(\"Ugly\", avg_counter)\n",
    "    print(\"Bad\", bad_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good 74\n",
      "Ugly 0\n",
      "Bad 0\n"
     ]
    }
   ],
   "source": [
    "give_confidence_test_set_images(model, test_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lus.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
